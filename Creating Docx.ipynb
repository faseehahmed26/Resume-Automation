{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d9cf05b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting docx2pdf\n",
      "  Downloading docx2pdf-0.1.8-py3-none-any.whl (6.7 kB)\n",
      "Requirement already satisfied: pywin32>=227 in c:\\users\\fasee\\anaconda3\\lib\\site-packages (from docx2pdf) (302)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in c:\\users\\fasee\\anaconda3\\lib\\site-packages (from docx2pdf) (4.64.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\fasee\\anaconda3\\lib\\site-packages (from tqdm>=4.41.0->docx2pdf) (0.4.5)\n",
      "Installing collected packages: docx2pdf\n",
      "Successfully installed docx2pdf-0.1.8\n"
     ]
    }
   ],
   "source": [
    "!pip install docx2pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b67ccd14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from typing import Dict, Tuple, Optional\n",
    "import logging\n",
    "\n",
    "from typing import Dict\n",
    "from docx import Document\n",
    "from docx import Document\n",
    "from typing import Dict\n",
    "import re\n",
    "import docx\n",
    "from docx import Document\n",
    "from docx.shared import Pt, Inches\n",
    "from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
    "from docx.shared import RGBColor\n",
    "import os\n",
    "from docx2pdf import convert\n",
    "from typing import Dict\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "import os\n",
    "from docx.oxml import OxmlElement\n",
    "from docx.oxml.ns import qn\n",
    "from docx.shared import Pt\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "print() #key should now be available\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "client =  OpenAI(\n",
    "    api_key= os.environ.get(\"OPENAI_API_KEY\"),\n",
    "organization='org-Qf0ZgkD7gMaJ8A44ouvmZkvt',\n",
    ")\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "2e949b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx.oxml import OxmlElement\n",
    "from docx.oxml.ns import qn\n",
    "\n",
    "def add_hyperlink(paragraph, url, text):\n",
    "    \"\"\"\n",
    "    Add a hyperlink to a paragraph in a docx document.\n",
    "    :param paragraph: The paragraph to which the hyperlink will be added.\n",
    "    :param url: The URL for the hyperlink.\n",
    "    :param text: The text to display for the hyperlink.\n",
    "    \"\"\"\n",
    "    # Create the relationship for the hyperlink\n",
    "    part = paragraph._parent.part\n",
    "    r_id = part.relate_to(\n",
    "        url, \"http://schemas.openxmlformats.org/officeDocument/2006/relationships/hyperlink\", is_external=True\n",
    "    )\n",
    "\n",
    "    # Create the hyperlink tag\n",
    "    hyperlink = OxmlElement(\"w:hyperlink\")\n",
    "    hyperlink.set(qn(\"r:id\"), r_id)\n",
    "\n",
    "    # Create the run for the hyperlink text\n",
    "    new_run = OxmlElement(\"w:r\")\n",
    "    rPr = OxmlElement(\"w:rPr\")\n",
    "    rStyle = OxmlElement(\"w:rStyle\")\n",
    "    rStyle.set(qn(\"w:val\"), \"Hyperlink\")  # Apply the \"Hyperlink\" style\n",
    "    rPr.append(rStyle)\n",
    "    new_run.append(rPr)\n",
    "    text_element = OxmlElement(\"w:t\")\n",
    "    text_element.text = text\n",
    "    new_run.append(text_element)\n",
    "    hyperlink.append(new_run)\n",
    "\n",
    "    # Append the hyperlink element to the paragraph\n",
    "    paragraph._element.append(hyperlink)\n",
    "    return paragraph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34293042",
   "metadata": {},
   "outputs": [],
   "source": [
    "docx_path1=\"./Faseeh Resume GA11.docx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe5b34a9",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "NAME:\n",
      "\n",
      "==================================================\n",
      "CONTACT:\n",
      "\n",
      "email: mm9314@g.rit.edu\n",
      "\n",
      "phone: +1 585 202 5217\n",
      "\n",
      "LinkedIn: https://www.linkedin.com/in/mohammad-faseeh-ahmed/\n",
      "\n",
      "Github: https://github.com/faseehahmed26\n",
      "\n",
      "Portfolio: https://faseehahmed26.github.io/portfolio/\n",
      "\n",
      "Kaggle: https://www.kaggle.com/faseeh001\n",
      "\n",
      "Tableau: https://public.tableau.com/app/profile/faseeh5112\n",
      "\n",
      "==================================================\n",
      "EDUCATION:\n",
      "\n",
      "Rochester Institute of Technology, Rochester, NY, M.S in Data Science\t     Aug 2023 - May 2025\n",
      "\n",
      "Coursework: Neural Networks, Software Engineering for Data Science, Applied Statistics\t\t   GPA: 3.84/4.00\n",
      "\n",
      "Jawaharlal Nehru Technological University Hyderabad, B.Tech in Computer Science\t   July 2018 - July 2022\n",
      "\n",
      "Coursework: Data Structures and Algorithms, Computer Vision, Data Warehousing, NLP \t            GPA: 3.2/4.00\n",
      "\n",
      "==================================================\n",
      "SKILLS:\n",
      "\n",
      "Programming Languages: Java, Python, C++, R, JavaScript, Go, Julia,  Object Oriented Programming(Python, Java)\n",
      "\n",
      "Frameworks: PyTorch, Keras, Scikit, Tensorflow, Groovy, Spark, Flask, React, React-Native, NodeJS,\n",
      "\n",
      "Databases: SQL, MongoDB, SQLite, MySQL, NoSQL, PostgreSQL, DynamoDB, SAS\n",
      "\n",
      "Technologies: Docker, Git, AWS, Azure, GCP, Kafka, JSON, Numpy, Pandas, MLflow, Postman, Tableau,Power BI, MS Excel\n",
      "\n",
      "ML Algorithms/Techniques: Regression, Classification, Clustering, Recommender Systems, Deep Learning, NLP, A/B\n",
      "\n",
      "==================================================\n",
      "PROFESSIONAL EXPERIENCE AND INTERNSHIPS:\n",
      "\n",
      "title: Daiichi Sankyo Inc, Basking Ridge, NJ- R&D Data Governance Intern\t        06/2024 - Present\n",
      "\n",
      "points: ['Developed an ICF analysis tool using BERT and T5 models, also testing Amazon Bedrock for text processing.', 'Built an on-premise chatbot using Flask, delivering clinical trial data 70% faster using RAG and Ollama architecture.', 'Enabled dynamic visualizations and automated milestone tracking, reducing missed deadlines by 30% and improving decision-making efficiency.', 'Built a data migration pipeline from Veeva Vault to Amazon Redshift, maintaining data integrity and indexing for', 'high-performance analytics.', 'Implemented data governance checks within the migration process, upholding compliance standards and ensuring', 'reliable, policy-aligned data access.', 'Rochester Institute Of Technology, Rochester, NY- Research Assistant\\t        08/2024 - Present', 'Collaborating with Professor Haibo Yang to enhance federated learning models, utilizing gRPC and PyTorch to implement scalable decentralized training algorithms.', 'Working on advanced distributed training techniques for Large Language Models (LLMs) on GPU, achieving a 30% increase in system performance and minimizing network latency.']\n",
      "\n",
      "title: SEO Content AI, Los Angeles, CA - AI Infrastructure Engineer  \t        11/2022 - 07/2023\n",
      "\n",
      "points: ['Utilized Docker for scalable deployment across cloud environments, focusing on AWS ECS and Fargate.', 'Optimized content creation through A/B testing with models like LLaMA-13B, T5, and GPT-3.5.', 'Incorporated Langchain, LLamaIndex and Vector Databases like Pinecone and FAISS, for advanced AI-driven search capabilities, significantly enhancing the precision and relevance of content generation.', 'Participated in the full SDLC, from design to iterative development and unit testing, within an agile team framework.']\n",
      "\n",
      "title: White Label Resell, Los Angeles, CA - Machine Learning Engineer                                                 06/2022 - 03/2023\n",
      "\n",
      "points: ['Deployed API endpoints with NLP, securing over 130,000 quality article generations in a week.', 'Employed MLOps practices with Git and Docker for streamlined model development and deployment.', 'Explored generative models (GPT-3, GPT-Neo, BERT) to diversify content creation, enhancing article uniqueness.']\n",
      "\n",
      "title: Digital Clinics Research and Services, Hyderabad, India - Data Scientist Intern\t\t 11/2021 - 12/2022\n",
      "\n",
      "points: ['Developed an image classification system using Faster R-CNN for detecting cancerous cells in histopathological images.']\n",
      "\n",
      "title: Engineered a segmentation solution with Detectron2 for precise tumor delineation in medical scans.\n",
      "\n",
      "points: ['Implemented YOLOv5 for automated screening of pathological slides, improving diagnostic speed and accuracy.']\n",
      "\n",
      "title: Edgeforce Solutions, Hyderabad, India - Data Scientist Intern\t       11/2021 - 02/2022\n",
      "\n",
      "points: ['Built a YOLOv5-based real-time object detection system using Python and TensorFlow, integrated with Streamlit for a user-friendly interface, achieving 90% accuracy.']\n",
      "\n",
      "title: Engineered a deep learning model for speech recognition, deployed in an Army Walkie Talkie emulator.\n",
      "\n",
      "points: []\n",
      "\n",
      "==================================================\n",
      "PROJECTS:\n",
      "\n",
      "name: Chronic Kidney Disease Predictor\n",
      "\n",
      "points: []\n",
      "\n",
      "name: Achieved a 98% F1-score in kidney disease prediction using Python's Pandas, Numpy, and Scikit-learn for data manipulation and used cross validation for  model development.\n",
      "\n",
      "points: []\n",
      "\n",
      "name: Orchestrated a hybrid Full Stack End-to-End application, utilizing Flask for backend and the MERN stack for frontend.\n",
      "\n",
      "points: []\n",
      "\n",
      "name: Number Plate Detection\n",
      "\n",
      "points: []\n",
      "\n",
      "name: Implemented SSD with OpenCV and OCR for accurate, real-time number plate detection and text extraction.\n",
      "\n",
      "points: []\n",
      "\n",
      "==================================================\n",
      "RESEARCH PAPERS:\n",
      "\n",
      "title: Tubules Detection on Breast Carcinoma Whole Slide Images Using Artificial Intelligence (Sep 2022):\n",
      "\n",
      "description: The abstract was selected for presentation at the prestigious International C-MIMI Conference at Johns Hopkins University, underscoring the project's contribution to medical imaging and diagnostics.\n",
      "\n",
      "title: Web-Based Mitosis Detection on Breast Cancer Whole Slide Images Using FasterRCNN and YOLOv5 (Dec 2022):\n",
      "\n",
      "description: This significant paper, published by the Science and Information (SAI) Organization in the International Journal of Advanced Computer Science Applications (IJACA).\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "doc = Document(docx_path1)\n",
    "cv_data = extract_cv_data(doc)\n",
    "\n",
    "# Print extracted data in a structured way\n",
    "for section, content in cv_data.items():\n",
    "    print(f\"\\n{'='*50}\\n{section.upper()}:\")\n",
    "    if isinstance(content, dict):\n",
    "        for key, value in content.items():\n",
    "            print(f\"\\n{key}: {value}\")\n",
    "    elif isinstance(content, list):\n",
    "        for item in content:\n",
    "            if isinstance(item, dict):\n",
    "                for k, v in item.items():\n",
    "                    print(f\"\\n{k}: {v}\")\n",
    "            else:\n",
    "                print(f\"\\n{item}\")\n",
    "job_description = \"Your job description here\"\n",
    "prompts = create_resume_prompts(cv_data, job_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "52c1893c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n        Given the job description and current skills, select the most relevant ones for each category.\\n        Keep the same categories but optimize content within each to match job requirements.\\n        Each category should fit on one line (10-16 words).\\n        \\n        Job Description: Your job description here\\n        Current Skills: {'Programming Languages': 'Java, Python, C++, R, JavaScript, Go, Julia,  Object Oriented Programming(Python, Java)', 'Frameworks': 'PyTorch, Keras, Scikit, Tensorflow, Groovy, Spark, Flask, React, React-Native, NodeJS,', 'Databases': 'SQL, MongoDB, SQLite, MySQL, NoSQL, PostgreSQL, DynamoDB, SAS', 'Technologies': 'Docker, Git, AWS, Azure, GCP, Kafka, JSON, Numpy, Pandas, MLflow, Postman, Tableau,Power BI, MS Excel', 'ML Algorithms/Techniques': 'Regression, Classification, Clustering, Recommender Systems, Deep Learning, NLP, A/B'}\\n        \\n        Return format: same skills dictionary structure with optimized content.\\n        \""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts['skills']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d52cdf4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linkedin  linkedin https://www.linkedin.com/in/mohammad-faseeh-ahmed/\n",
      "https://www.linkedin.com/in/mohammad-faseeh-ahmed/\n",
      "github github https://github.com/faseehahmed26\n",
      "https://github.com/faseehahmed26\n",
      "portfolio portfolio https://faseehahmed26.github.io/portfolio/\n",
      "https://faseehahmed26.github.io/portfolio/\n",
      "kaggle kaggle https://www.kaggle.com/faseeh001\n",
      "https://www.kaggle.com/faseeh001\n",
      "tableau tableau https://public.tableau.com/app/profile/faseeh5112\n",
      "https://public.tableau.com/app/profile/faseeh5112\n",
      "{'Name': 'Mohammad Faseeh Ahmed', 'Contact': {'email': 'mm9314@g.rit.edu', 'phone': '+1 585 202 5217', 'links': {'LinkedIn': 'https://www.linkedin.com/in/mohammad-faseeh-ahmed/', 'Github': 'https://github.com/faseehahmed26', 'Portfolio': 'https://faseehahmed26.github.io/portfolio/', 'Kaggle': 'https://www.kaggle.com/faseeh001', 'Tableau': 'https://public.tableau.com/app/profile/faseeh5112'}}, 'Education': ['Rochester Institute of Technology, Rochester, NY,  M.S in Data Science\\t      Expected May 2025', 'Coursework: Neural Networks, Software Engineering for Data Science, Applied Statistics.                         GPA: 3.84/4.00', 'Jawaharlal Nehru Technological University Hyderabad, B.Tech in Computer Science\\t July 2018 - July 2022', 'Coursework: Data Structures and Algorithms, Computer Vision, Artificial Intelligence, NLP \\t         GPA: 3.2/4.00'], 'Skills': {'Programming Languages': 'Java, Python, C++, R, JavaScript, Object Oriented Programming(Python, Java)', 'Frameworks': 'PyTorch, Keras, Scikit, Tensorflow, Groovy, PySpark, Flask, React, NodeJS,', 'Databases': 'SQL, MongoDB, SQLite, MySQL, NoSQL, PostgreSQL, DynamoDB', 'Technologies': '', 'ML Algorithms/Techniques': 'Regression, Classification, Clustering, Recommender Systems, Deep Learning, NLP, A/B Testing, Time Series, Optimization, EDA, ETL,'}, 'Professional Experience And Internships': [{'title': 'Daiichi Sankyo Inc, Basking Ridge, NJ- R&D Data Governance Intern\\t        03/2024 - Present', 'points': ['Developed an Informed Consent Forms (ICF) analysis tool using BERT and T5 models, also testing Amazon Bedrock for large-scale language processing.', 'Built a Flask-based frontend for the ICF tool, enabling secure modifications of document classifications based on user permissions.', 'Implemented advanced Large Language Models (LLMs) using Amazon Bedrock, achieving a 20% increase in classification accuracy for detecting explicit prohibitions against data sharing.', 'Utilized Amazon SageMaker for model training and fine-tuning, streamlining the handling of vast volumes of legal documents to ensure scalability and efficiency.', 'Optimized model inference and processing times with EC2 instances, improving overall performance and reliability of the ICF analysis tool.', '1. Designed an on-premise chatbot delivering instant trial data, cutting access time by 70%.', '2. Enabled visualizations in the chatbot, improving data comprehension and decision-making efficiency.', '3. Automated milestone and site tracking, enhancing study performance and reducing missed deadlines by 30%.', '4. Developed a secure chatbot architecture, ensuring 100% compliance with company data regulations.', '5. Integrated RAG and AI, achieving 80% improvement in answer accuracy and user trust.', 'Led the migration of clinical data from Veeva Vault to Redshift, establishing an ETL pipeline that ensured data accuracy and improved analytics accessibility.', 'Built indexing and foreign key relationships in Redshift to optimize query performance, enabling high-speed analysis on clinical data.', 'Implemented data validation checks during migration to uphold data governance and regulatory compliance, ensuring reliable and accurate data in the new environment.', \"Collaborating with stakeholders to align the tool's capabilities with Daiichi Sankyo's data-sharing policies, compliance standards, and real-world data requirements.\", 'Rochester Institute Of Technology, Rochester, NY- Research Assistant\\t        08/2024 - Present', 'Collaborating with Professor Haibo Yang to enhance federated learning models, utilizing gRPC and PyTorch to implement scalable decentralized training algorithms.', 'Analyzing and optimizing distributed environments with PyTorch RPC, reducing communication overhead by 15% through improved data transfer protocols.', 'Leading the integration of the FedDisco algorithm into existing frameworks, accelerating model convergence and reducing training time by 20%.', \"Transitioning between distributed communication algorithms, leveraging FedDisco's adaptive communication strategies for efficient, real-time model updates.\", 'Developing advanced distributed training techniques for Large Language Models (LLMs), achieving a 30% increase in system performance and minimizing network latency.', 'Utilizing gRPC for dynamic, secure communication within federated learning models, enhancing data synchronization and increasing overall model accuracy by 10%.', 'Designing hands-on projects for the Intro to Machine Learning course, applying theory to real-world machine learning scenarios using Python and PyTorch.', 'Providing personalized support to over 25 students, fostering problem-solving skills and guiding successful project outcomes, significantly improving their understanding of distributed machine learning concepts.']}, {'title': 'SEO Content AI, Los Angeles, CA - AI Infrastructure Engineer  \\t Nov 2022 - July 2023', 'points': ['Enhanced AI-driven content generation efficiency by 25% through leading the integration of transformers within AWS microservices.', 'Developed a Chrome extension using Python, JavaScript, and NodeJS, boosting content quality and generation speed by 40%.', 'Utilized Docker for scalable and reliable deployment across cloud environments via AWS ECS and Fargate.', 'Improved project management and team collaboration through implementing effective development methodologies.', 'Optimized content creation for various use cases by conducting A/B testing with models like LLaMA-13B, Flan-T5, Vicuna-13B, and GPT-3.5.', \"Broadened audience reach by using GCP's Google Translate API for multi-language content translation.\", 'Ensured high-quality, error-free content by implementing NLP algorithms with BERT, RoBERTa, and DistilBERT for grammar correction, significantly enhancing user trust.']}, {'title': 'White Label Resell, Los Angeles, CA - Machine Learning Engineer                                      June 2022 - March 2023', 'points': ['Automated article generation for digital marketing using AWS Lambda, NodeJS, and API Gateway achieving a 60x reduction in operational costs.', 'Integrated NLP and TensorFlow to analyze and generate over 130K high-quality articles within a week, improving content strategy.', 'Utilized DynamoDB for managing large datasets, ensuring efficient data access and manipulation for content generation processes.', 'Fine-tuned advanced NLP models like BERT, RoBERTa, ALBERT, and DistilBERT for contextual understanding, improving the relevance and quality of generated articles.', 'Explored generative models including GPT-3, GPT-Neo, GPT-J, and GPT-NeoX, to diversify content creation, ensuring a wide range of engaging and unique articles.', 'Leveraged Flan-T5 and T5 Codegen for specific content generation tasks, optimizing for both efficiency and creativity in article production.', 'Developed custom dataset models with Gensim, enhancing thematic accuracy and alignment with marketing goals.', 'Employed MLOps practices with Git and Docker to streamline the development, training, and deployment of machine learning models, facilitating continuous improvement and collaboration.']}, {'title': 'Digital Clinics Research and Services, Hyderabad, India - Data Scientist Intern\\tNov 2021 - Dec 2022', 'points': ['Developed a sophisticated image classification system using Faster R-CNN, tailored for the nuanced detection of cancerous cells in histopathological images, leveraging TensorFlow for model training and optimization.']}, {'title': 'Engineered a custom segmentation solution with Detectron2, enabling precise delineation of tumor boundaries in medical scans. This approach utilized QuPath for image management and Groovy for scripting complex analysis workflows, significantly enhancing the quality of medical image analysis.', 'points': ['Implemented YOLOv5 for the automated screening of pathological slides, achieving unprecedented speed and accuracy in identifying critical diagnostic markers, integrated with OpenCV for real-time image processing and enhancements.', 'Led the integration of MLOps practices, utilizing Docker for containerization and AWS for model deployment, ensuring scalable and robust AI solutions in clinical settings.', \"Mentored a diverse group of 10-20 junior colleagues (first and second-year students) on multiple ML and DL projects, fostering a collaborative learning environment and enhancing the team's overall AI expertise and project execution capabilities.\"]}, {'title': 'Edgeforce Solutions, Hyderabad, India - Data Scientist Intern\\tNov 2021 - Feb 2022', 'points': ['Built a YOLOv5-based real-time object detection system using Python and TensorFlow, integrated with Streamlit for a user-friendly interface, achieving 90% accuracy.']}, {'title': 'Engineered a deep learning model for speech recognition with PyTorch, achieving 95% accuracy, deployed in an Army Walkie Talkie emulator.', 'points': ['Utilized Docker and AWS for deploying scalable and efficient machine learning models, reducing operational costs and improving resource utilization.', 'Leveraged SQL and SQLite for efficient data storage and retrieval, enhancing system performance and data management.', \"Conducted A/B testing to optimize the model's performance, resulting in enhanced accuracy and user satisfaction.\"]}], 'Projects': [{'name': 'Chronic Kidney Disease Predictor \\t Nov 2022 - July 20', 'points': []}, {'name': \"Utilized Python's Pandas and Numpy for data manipulation and preprocessing, ensuring accurate model inputs.\", 'points': []}, {'name': 'Implemented Scikit-learn to develop a logistic regression model, achieving a 98% F1 score for kidney disease prediction.', 'points': []}, {'name': 'Orchestrated a hybrid application structure, employing Flask for backend functionalities and leveraging the MERN stack for frontend development, to provide an intuitive platform for disease prediction.', 'points': []}, {'name': 'Employed Matplotlib and Seaborn for data visualization, providing insightful analytics for model performance evaluation.', 'points': []}, {'name': 'Applied MongoDB for efficient data storage, enabling scalable and secure management of patient data.', 'points': []}, {'name': 'Beverage Management System                                                                                        June 2022 - March 2023', 'points': []}, {'name': 'Implemented TensorFlow for creating a convolutional neural network model, achieving real-time beverage detection with 98% accuracy.', 'points': []}, {'name': \"Used OpenCV for image processing, enhancing the model's ability to recognize various beverages under different conditions.\", 'points': []}, {'name': 'Developed a web application using Flask as the backend and ReactJS for the frontend, offering a seamless user experience.', 'points': []}, {'name': 'Leveraged PostgreSQL for database management, ensuring robust and reliable storage of inventory data.', 'points': []}, {'name': 'Incorporated Docker for deploying the application, ensuring consistency across development and production environments.', 'points': []}, {'name': 'Covid19 Bot  \\t Nov 2022 - July 20', 'points': []}, {'name': 'Developed a chatbot using Python, Flask, and DialogFlow to offer real-time COVID-19 updates via Telegram, enhancing accessibility for users.', 'points': []}, {'name': 'Implemented Pandas and Numpy for robust data analysis and numerical computations to provide accurate and up-to-date health statistics.', 'points': []}, {'name': 'Integrated MongoDB to efficiently store and manage user queries and responses, facilitating personalized interaction and data retrieval.', 'points': []}, {'name': \"Utilized RapidAPI for accessing live COVID-19 data, ensuring the chatbot's information was current and reliable for users seeking instant updates.\", 'points': []}, {'name': 'Flight Price Predictor  \\t Nov 2022 - July 20', 'points': []}, {'name': \"Enhanced the predictive model using Scikit-learn's regression techniques and LSTM networks for dynamic price forecasting, incorporating temporal data analysis.\", 'points': []}, {'name': 'Conducted extensive feature engineering with Pandas, improving model accuracy by identifying key price determinants from historical flight data.', 'points': []}, {'name': 'Utilized Matplotlib and Seaborn for insightful visualizations of data trends and model performance, aiding in the interpretability of predictions.', 'points': []}, {'name': 'Deployed the enhanced model on Heroku with a Flask web application interface, providing users with real-time, accurate flight price predictions.', 'points': []}, {'name': 'Generating Synthetic Anime Data                                                                                        June 2022 - March 2023', 'points': []}, {'name': 'Leveraged PyTorch and a DC GAN architecture to create high-quality synthetic anime images, significantly enhancing the diversity of our machine learning datasets.', 'points': []}, {'name': \"Utilized OpenCV for advanced image preprocessing and augmentation tasks, including resizing, normalization, and color adjustments, to prepare images for training, enhancing the model's ability to generate diverse and realistic anime characters.\", 'points': []}, {'name': 'Developed Python scripts to automate the collection and preprocessing of anime images, streamlining the dataset creation process and enabling the efficient generation of large volumes of training data.', 'points': []}, {'name': \"Applied OpenCV's edge detection and image filtering capabilities to enhance the detail and quality of generated images, ensuring a high degree of realism and variability in the synthetic data.\", 'points': []}, {'name': 'Integrated additional data augmentation techniques such as flipping, rotation, and scaling through OpenCV, further augmenting the training dataset to improve the robustness and generalization of the GAN model.', 'points': []}, {'name': 'Number Plate Detection  \\t Nov 2022 - July 20', 'points': []}, {'name': 'Deployed YOLOv5 for real-time number plate detection, achieving high accuracy in diverse lighting and environmental conditions.', 'points': []}, {'name': 'Incorporated OpenCV for advanced image processing tasks, such as adaptive thresholding and contour detection, to improve the accuracy of number plate recognition.', 'points': []}, {'name': 'Developed a system using Flask as the backend, enabling real-time detection and information processing, with a user-friendly interface for monitoring and alerts.', 'points': []}, {'name': \"Integrated OpenCV's Gaussian Blur to reduce image noise and improve the model's detection capabilities in varying environmental conditions.\", 'points': []}, {'name': \"Implemented perspective transformation techniques in OpenCV to correct angles and enhance the readability of number plates, ensuring accurate recognition regardless of the vehicle's orientation.\", 'points': []}, {'name': 'Research Papers', 'points': []}, {'name': 'Tubules Detection on Breast Carcinoma Whole Slide Images Using Artificial Intelligence (Sep 2022):', 'points': []}, {'name': \"This groundbreaking research utilized advanced AI deep learning models to accurately detect tubules in breast carcinoma images. The abstract was recognized for its innovation and selected for presentation at the prestigious International C-MIMI Conference at Johns Hopkins University, underscoring the project's contribution to medical imaging and diagnostics.\", 'points': []}, {'name': 'Web-Based Mitosis Detection on Breast Cancer Whole Slide Images Using FasterRCNN and YOLOv5 (Dec 2022):', 'points': []}, {'name': 'This significant paper, published by the Science and Information (SAI) Organization in the International Journal of Advanced Computer Science Applications (IJACA), detailed the development and application of Faster R-CNN and YOLOv5 for the detection of mitosis in breast cancer whole slide images. The research provided insights into the capabilities of these models in enhancing the accuracy and efficiency of breast cancer grading, demonstrating a pivotal step forward in the application of machine learning in healthcare.', 'points': []}, {'name': 'Kaggle Notebook and Dataset Expert', 'points': []}, {'name': 'Open Source Contributor at OpenVino(Intel), Ivy', 'points': []}, {'name': \"Won Best Overall Hack in CSH Hacks Hackathon 2023 : Developed a Chrome extension with a streamlined front-end, leveraging AWS Lambda for backend deployment, which integrates OpenAI's GPT-3.5 to scrutinize terms and conditions, enhancing user vigilance through AWS-powered processing and API Gateway for efficient data handling, encapsulating cloud architecture expertise in delivering a user-centric solution.\", 'points': []}], 'Research Papers': []}\n"
     ]
    }
   ],
   "source": [
    " # Initialize OpenAI client\n",
    "docx_path = \"./Faseeh Curriculum Vitae.docx\"\n",
    "\n",
    "# Load your resume\n",
    "doc = Document(docx_path)\n",
    "cv_data = extract_cv_data(doc)  # Using the extract_cv_data function from earlier\n",
    "\n",
    "job_description = '''LinkedIn logo\n",
    "LinkedIn\n",
    "Share\n",
    "Show more options\n",
    "AI Engineer \n",
    "Sunnyvale, CA · 3 weeks ago · Over 100 applicants\n",
    "$111K/yr - $183K/yrMatches your job preferences, minimum pay preference is 140000.  HybridMatches your job preferences, workplace type is Hybrid.  Full-timeMatches your job preferences, job type is Full-time.\n",
    "21 connections work here · 10 company alumni work here · 32 school alumni work here\n",
    "\n",
    "\n",
    "Am I a good fit?\n",
    "\n",
    "Tailor my resume\n",
    "\n",
    "How can I best position myself?\n",
    "\n",
    "Easy Apply\n",
    "\n",
    "Saved\n",
    "Saved AI Engineer  at LinkedIn\n",
    "Stand out to the employer by marking this job as a top choice when you apply. Learn more\n",
    "About the job\n",
    "LinkedIn is the world’s largest professional network, built to help members of all backgrounds and experiences achieve more in their careers. Our vision is to create economic opportunity for every member of the global workforce. Every day our members use our products to make connections, discover opportunities, build skills and gain insights. We believe amazing things happen when we work together in an environment where everyone feels a true sense of belonging, and that what matters most in a candidate is having the skills needed to succeed. It inspires us to invest in our talent and support career growth. Join us to challenge yourself with work that matters.\n",
    "\n",
    "This role will be based in Sunnyvale, San Francisco, Bellevue or New York City.\n",
    "\n",
    "At LinkedIn, we trust each other to do our best work where it works best for us and our teams. This role offers a hybrid work option, meaning you can work from home and commute to a LinkedIn office, depending on what’s best for you and when it is important for your team to be together. This is a full-time engineering role based in Sunnyvale, San Francisco, Bellevue or New York City.\n",
    "LinkedIn's Machine Learning Engineers are both data/research scientists and software engineers, who develop and implement machine learning models and algorithms. Unlike other companies that separate these roles, our engineers work on projects from ideation to implementation. \n",
    "\n",
    "Responsibilities:\n",
    "• Building and deploying relevance models that power our various recommender systems and personalized intent engines\n",
    "• Keeping scalability and performance in mind through your design and engineering choices.\n",
    "• You will explore novel approaches for machine learning ranking and modeling downstream impact, as well as new ways of creating new product features by leveraging the cutting edge machine learning techniques in Generative AI, NLP.\n",
    "• You will work with big data and analyze tracking data to understand member preferences and identify opportunities for taking the products you work on to the next level.\n",
    "• You will work directly with partner data science, product, and infrastructure teams to take your ideas from conception to production.\n",
    "\n",
    "Basic Qualifications:\n",
    "• Bachelor's degree in Computer Science or related technical field or equivalent technical experience\n",
    "• 1+ years experience with machine learning, data mining, and information retrieval or natural language processing \n",
    "• 1+ year of industry experience in software design, development, and algorithm related solutions.\n",
    "• 1+ years experience in programming languages such as Java, Python, etc.\n",
    "\n",
    "Preferred Qualifications: \n",
    "● 1+ years of relevant work experience\n",
    "● MS or PhD in Computer Science or related technical discipline\n",
    "● Experience developing large scale systems\n",
    "● Knowledge of recommendation systems\n",
    "● Ability to diagnose technical problems, debug code, and automate routine tasks\n",
    "● Analytical approach coupled with solid communication skills and a sense of ownership\n",
    "• Experience in Machine Learning and Deep Learning\n",
    "• Experience in Big Data \n",
    "• Strong technical background & Strategic thinking\n",
    "\n",
    "Suggested Skills:\n",
    "• Data Mining\n",
    "• Data Coding\n",
    "• Machine Learning\n",
    "\n",
    "'''\n",
    "\n",
    "print(cv_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "18e8fc8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Content\n",
      "{'skills': '```json\\n{\\n    \"Programming Languages\": \"Java, Python\",\\n    \"Frameworks\": \"PyTorch, Keras, Scikit-learn, TensorFlow, PySpark, Flask\",\\n    \"Databases\": \"SQL, NoSQL, MongoDB, DynamoDB\",\\n    \"Technologies\": \"Big Data\",\\n    \"ML Algorithms/Techniques\": \"Recommender Systems, Deep Learning, NLP, Regression, Classification, Clustering\"\\n}\\n```', 'experience': '```json\\n{\\n    \"experiences\": [\\n        {\\n            \"full_title\": \"Daiichi Sankyo Inc, Basking Ridge, NJ - R&D Data Governance Intern    03/2024 - Present\",\\n            \"relevance_score\": \"high\",\\n            \"points\": [\\n                \"Developed an ICF analysis tool using **BERT**, **T5**, and **Amazon Bedrock**, enhancing language processing capabilities.\",\\n                \"Built a **Flask** frontend for the ICF tool, enabling secure document classification modifications based on user permissions.\",\\n                \"Implemented **Amazon Bedrock** LLMs, increasing classification accuracy by 20% in detecting data sharing prohibitions.\",\\n                \"Utilized **Amazon SageMaker** for model training, streamlining the handling of large legal document volumes efficiently.\"\\n            ]\\n        },\\n        {\\n            \"full_title\": \"SEO Content AI, Los Angeles, CA - AI Infrastructure Engineer    Nov 2022 - July 2023\",\\n            \"relevance_score\": \"high\",\\n            \"points\": [\\n                \"Enhanced AI content generation by 25% through integrating **transformers** within **AWS** microservices.\",\\n                \"Developed a **Chrome extension** using **Python**, **JavaScript**, and **NodeJS**, boosting content speed by 40%.\",\\n                \"Utilized **Docker**, **AWS ECS**, and **Fargate** for scalable and reliable deployment across cloud environments.\",\\n                \"Implemented **NLP** algorithms with **BERT**, **RoBERTa**, and **DistilBERT**, enhancing content quality and user trust.\"\\n            ]\\n        },\\n        {\\n            \"full_title\": \"White Label Resell, Los Angeles, CA - Machine Learning Engineer    June 2022 - March 2023\",\\n            \"relevance_score\": \"high\",\\n            \"points\": [\\n                \"Automated article generation using **AWS Lambda**, **NodeJS**, and **API Gateway**, reducing operational costs by 60x.\",\\n                \"Integrated **NLP** and **TensorFlow** to generate over 130K articles weekly, improving content strategy effectiveness.\",\\n                \"Fine-tuned **BERT**, **RoBERTa**, and **ALBERT** models, enhancing the relevance and quality of generated content.\",\\n                \"Implemented **MLOps** with **Git** and **Docker**, streamlining ML model development, training, and deployment.\"\\n            ]\\n        },\\n        {\\n            \"full_title\": \"Rochester Institute Of Technology, Rochester, NY - Research Assistant    08/2024 - Present\",\\n            \"relevance_score\": \"medium\",\\n            \"points\": [\\n                \"Enhanced federated learning models using **gRPC** and **PyTorch**, implementing scalable decentralized algorithms.\",\\n                \"Optimized distributed environments with **PyTorch RPC**, reducing communication overhead by 15%.\",\\n                \"Integrated the **FedDisco** algorithm, accelerating model convergence and reducing training time by 20%.\",\\n                \"Developed advanced distributed training techniques for **LLMs**, increasing system performance by 30%.\"\\n            ]\\n        },\\n        {\\n            \"full_title\": \"Digital Clinics Research and Services, Hyderabad, India - Data Scientist Intern    Nov 2021 - Dec 2022\",\\n            \"relevance_score\": \"medium\",\\n            \"points\": [\\n                \"Developed an image classification system using **Faster R-CNN** with **TensorFlow**, enhancing cancer cell detection accuracy.\",\\n                \"Engineered a segmentation solution with **Detectron2** and **QuPath**, improving tumor boundary delineation in medical scans.\",\\n                \"Implemented **YOLOv5** integrated with **OpenCV**, achieving rapid and accurate pathological slide screening.\",\\n                \"Deployed models using **Docker** and **AWS**, ensuring scalable and robust AI solutions in clinical settings.\"\\n            ]\\n        },\\n        {\\n            \"full_title\": \"Edgeforce Solutions, Hyderabad, India - Data Scientist Intern    Nov 2021 - Feb 2022\",\\n            \"relevance_score\": \"medium\",\\n            \"points\": [\\n                \"Built a **YOLOv5** real-time object detection system with **Python** and **TensorFlow**, achieving 90% accuracy.\",\\n                \"Integrated the system with **Streamlit** for a user-friendly interface, enhancing user interaction and accessibility.\"\\n            ]\\n        }\\n    ]\\n}\\n```', 'projects': '```json\\n{\\n    \"projects\": [\\n        {\\n            \"name\": \"Chronic Kidney Disease Predictor\",\\n            \"points\": [\\n                \"Utilized **Python\\'s Pandas** and **NumPy** for data preprocessing, ensuring accurate model inputs.\",\\n                \"Implemented **Scikit-learn** to develop a logistic regression model, achieving a 98% F1 score.\",\\n                \"Deployed with **Flask** and **MERN stack**, providing an intuitive platform for disease prediction.\"\\n            ]\\n        },\\n        {\\n            \"name\": \"Flight Price Predictor\",\\n            \"points\": [\\n                \"Enhanced model using **Scikit-learn** and **LSTM networks**, improving forecasting accuracy by 15%.\",\\n                \"Conducted feature engineering with **Pandas**, identifying key price determinants from historical data.\",\\n                \"Deployed on **Heroku** with a **Flask** interface, enabling real-time flight price predictions.\"\\n            ]\\n        }\\n    ]\\n}\\n```'}\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    " # Template styling\n",
    "\n",
    "# Process and save resume\n",
    "# docx_path, pdf_path = process_complete_resume(cv_data, job_description, client)\n",
    "# print(f\"Resume saved as:\\nDOCX: {docx_path}\\nPDF: {pdf_path}\")\n",
    "optimized_content = process_complete_resume(cv_data, job_description, client)\n",
    "# print(f\"Resume saved as:\\nDOCX: {docx_path}\\nPDF: {pdf_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "fb5ea2d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['skills', 'experience', 'projects'])"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimized_content.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "c7fc14f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```json\\n{\\n    \"projects\": [\\n        {\\n            \"name\": \"Chronic Kidney Disease Predictor\",\\n            \"points\": [\\n                \"Utilized **Python\\'s Pandas** and **NumPy** for data preprocessing, ensuring accurate model inputs.\",\\n                \"Implemented **Scikit-learn** to develop a logistic regression model, achieving a 98% F1 score.\",\\n                \"Deployed with **Flask** and **MERN stack**, providing an intuitive platform for disease prediction.\"\\n            ]\\n        },\\n        {\\n            \"name\": \"Flight Price Predictor\",\\n            \"points\": [\\n                \"Enhanced model using **Scikit-learn** and **LSTM networks**, improving forecasting accuracy by 15%.\",\\n                \"Conducted feature engineering with **Pandas**, identifying key price determinants from historical data.\",\\n                \"Deployed on **Heroku** with a **Flask** interface, enabling real-time flight price predictions.\"\\n            ]\\n        }\\n    ]\\n}\\n```'"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimized_content[\"projects\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "c344ae0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_style = {\n",
    "    \"name_size\": 16,\n",
    "    \"contact_size\": 9.5,\n",
    "    \"heading_size\": 10.5,\n",
    "    \"content_size\": 9\n",
    "}\n",
    "doc = generate_resume_docx(cv_data, optimized_content, template_style)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "6a387658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<docx.document.Document at 0x1bec1b148b0>"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "36603662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c5869adebf5428b8ccd0606ee9434d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "docx_path, pdf_path = save_resume(doc, \"Linkedin1\", cv_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "11a5024f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error parsing JSON: Expecting value: line 1 column 1 (char 0)\n"
     ]
    }
   ],
   "source": [
    "parse_json_content(\"optimized_content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9ea09edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json_content(json_str: str) -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Safely parse JSON string to dictionary\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Remove JSON code block markers if present\n",
    "        json_str = json_str.replace('```json', '').replace('```', '').strip()\n",
    "        return json.loads(json_str)\n",
    "    except json.JSONDecodeError as e:\n",
    "        logger.error(f\"Error parsing JSON: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b005257f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_content={'skills': '```json\\n{\\n    \"Programming Languages\": \"Java, Python, Object Oriented Programming(Python, Java)\",\\n    \"Frameworks\": \"PyTorch, Keras, Scikit, Tensorflow, Flask\",\\n    \"Databases\": \"SQL, MongoDB, MySQL, PostgreSQL, NoSQL\",\\n    \"Technologies\": \"\",\\n    \"ML Algorithms/Techniques\": \"Deep Learning, NLP, A/B Testing, Time Series\"\\n}\\n```', 'experience': '[\\n    {\\n        \"title\": \"Daiichi Sankyo Inc, Basking Ridge, NJ- R&D Data Governance Intern\\\\t03/2024 - Present\",\\n        \"points\": [\\n            \"Developed ICF analysis tool using BERT and T5 models, enhancing classification accuracy by 20%.\",\\n            \"Built Flask-based frontend for secure document modifications based on user permissions.\",\\n            \"Optimized model accuracy by 20% with Large Language Models using Amazon Bedrock.\",\\n            \"Utilized Amazon SageMaker for scalable legal document processing.\",\\n            \"Improved ICF analysis tool performance with EC2 instances.\",\\n            \"Designed on-premise chatbot reducing access time by 70%.\",\\n            \"Enabled visualizations for improved data comprehension.\",\\n            \"Automated milestone tracking, reducing missed deadlines by 30%.\",\\n            \"Developed secure chatbot architecture, ensuring 100% compliance with regulations.\",\\n            \"Integrated RAG and AI achieving 80% higher answer accuracy.\",\\n            \"Led clinical data migration to Redshift, ensuring data accuracy and analytics accessibility.\",\\n            \"Optimized Redshift query performance for high-speed analysis.\",\\n            \"Implemented data validation checks for migration compliance.\",\\n            \"Aligned tool capabilities with data-sharing policies and compliance standards.\"\\n        ]\\n    },\\n    {\\n        \"title\": \"Rochester Institute Of Technology, Rochester, NY- Research Assistant\\\\t08/2024 - Present\",\\n        \"points\": [\\n            \"Enhanced federated learning models with gRPC and PyTorch for scalable training algorithms.\",\\n            \"Reduced communication overhead by 15% in distributed environments with PyTorch RPC.\",\\n            \"Accelerated model convergence by 20% with FedDisco algorithm integration.\",\\n            \"Transitioned to adaptive communication strategies for real-time model updates.\",\\n            \"Improved system performance by 30% for Large Language Models.\",\\n            \"Enhanced federated learning model accuracy by 10% using gRPC.\",\\n            \"Designed real-world machine learning projects for student application.\",\\n            \"Provided personalized project support to foster student learning outcomes.\"\\n        ]\\n    },\\n    {\\n        \"title\": \"SEO Content AI, Los Angeles, CA - AI Infrastructure Engineer\\\\tNov 2022 - July 2023\",\\n        \"points\": [\\n            \"Boosted content generation efficiency by 25% through transformer integration.\",\\n            \"Developed Chrome extension for 40% faster content generation.\",\\n            \"Deployed Docker for scalable cloud deployment.\",\\n            \"Led effective development methodologies for team collaboration.\",\\n            \"Conducted A/B testing on various models for optimized content creation.\",\\n            \"Utilized GCP\\'s Google Translate API for multi-language content reach.\",\\n            \"Implemented NLP algorithms for error-free content and user trust.\"\\n        ]\\n    },\\n    {\\n        \"title\": \"White Label Resell, Los Angeles, CA - Machine Learning Engineer\\\\tJune 2022 - March 2023\",\\n        \"points\": [\\n            \"Achieved 60x cost reduction with automated article generation.\",\\n            \"Analyzed and generated 130K+ high-quality articles within a week.\",\\n            \"Managed large datasets efficiently with DynamoDB.\",\\n            \"Improved content relevance with advanced NLP model tuning.\",\\n            \"Diversified content creation with generative models.\",\\n            \"Optimized specific content generation tasks with T5 Codegen.\",\\n            \"Enhanced thematic accuracy with custom dataset models.\",\\n            \"Streamlined ML development with Git and Docker for continuous improvement.\"\\n        ]\\n    },\\n    {\\n        \"title\": \"Digital Clinics Research and Services, Hyderabad, India - Data Scientist Intern\\\\tNov 2021 - Dec 2022\",\\n        \"points\": [\\n            \"Developed image classification system with TensorFlow for cancerous cell detection.\"\\n        ]\\n    },\\n    {\\n        \"title\": \"Digital Clinics Research and Services, Hyderabad, India - Data Scientist Intern\\\\tNov 2021 - Dec 2022\",\\n        \"points\": [\\n            \"Engineered custom segmentation solution with Detectron2 for tumor boundary delineation.\",\\n            \"Implemented YOLOv5 for automated pathological slide screening with OpenCV integration.\",\\n            \"Led MLOps practices with Docker and AWS for scalable AI solutions.\",\\n            \"Mentored junior colleagues on ML and DL projects for enhanced expertise.\"\\n        ]\\n    },\\n    {\\n        \"title\": \"Edgeforce Solutions, Hyderabad, India - Data Scientist Intern\\\\tNov 2021 - Feb 2022\",\\n        \"points\": [\\n            \"Developed YOLOv5-based real-time object detection system with 90% accuracy.\"\\n        ]\\n    }\\n]'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "69ab90df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'company_name': 'Daiichi Sankyo Inc', 'location': 'Basking Ridge, NJ', 'title': 'R&D Data Governance Intern', 'dates': '03/2024 - Present', 'points': ['Developed ICF analysis tool with BERT, T5, increasing classification accuracy by 20%', 'Built Flask frontend for secure document classification based on user permissions', 'Utilized Amazon SageMaker for scalable model training on legal documents', 'Optimized model inference with EC2, enhancing tool performance and reliability']}, {'company_name': 'Rochester Institute Of Technology', 'location': 'Rochester, NY', 'title': 'Research Assistant', 'dates': '08/2024 - Present', 'points': ['Enhanced federated learning using gRPC and PyTorch, reducing communication overhead by 15%', 'Integrated FedDisco algorithm, accelerating model convergence and lowering training time by 20%', 'Developed distributed LLM training techniques, boosting system performance by 30%', 'Leveraged gRPC for secure communication, improving model accuracy by 10%']}, {'company_name': 'SEO Content AI', 'location': 'Los Angeles, CA', 'title': 'AI Infrastructure Engineer', 'dates': 'Nov 2022 - July 2023', 'points': ['Integrated transformers in AWS microservices, enhancing AI content generation by 25%', 'Developed Python and JavaScript Chrome extension, boosting content speed by 40%', 'Utilized Docker for scalable deployments on AWS ECS and Fargate', 'Implemented NLP algorithms with BERT, improving grammar correction accuracy']}, {'company_name': 'White Label Resell', 'location': 'Los Angeles, CA', 'title': 'Machine Learning Engineer', 'dates': 'June 2022 - March 2023', 'points': ['Automated article generation with AWS Lambda, NodeJS, reducing costs by 60x', 'Integrated TensorFlow NLP models, generating 130K+ articles weekly, enhancing content strategy', 'Utilized DynamoDB for efficient large dataset management in content processes', 'Fine-tuned BERT and GPT models, improving article relevance and quality']}, {'company_name': 'Digital Clinics Research and Services', 'location': 'Hyderabad, India', 'title': 'Data Scientist Intern', 'dates': 'Nov 2021 - Dec 2022', 'points': ['Developed image classification system with Faster R-CNN, enhancing cancer cell detection accuracy', 'Implemented YOLOv5 with OpenCV for real-time pathological slide analysis, increasing diagnostic speed', 'Leveraged Docker and AWS for scalable deployment, ensuring robust clinical AI solutions', 'Mentored 10+ junior colleagues on ML projects, improving team expertise and execution']}, {'company_name': 'Edgeforce Solutions', 'location': 'Hyderabad, India', 'title': 'Data Scientist Intern', 'dates': 'Nov 2021 - Feb 2022', 'points': ['Built YOLOv5-based real-time object detection with Python and TensorFlow, achieving 90% accuracy', 'Engineered speech recognition model with PyTorch, achieving 95% accuracy for Army Walkie Talkie emulator', 'Deployed models using Docker and AWS, reducing operational costs and improving efficiency', 'Utilized SQL for data management, enhancing system performance and data retrieval']}]\n",
      "{'SelectedProjects': [{'name': 'Chronic Kidney Disease Predictor', 'points': ['Implemented **Scikit-learn** to develop a **logistic regression model**, achieving a **98% F1 score** for kidney disease prediction.', \"Utilized **Python's Pandas and Numpy** for data manipulation and preprocessing, ensuring accurate model inputs.\", 'Orchestrated a hybrid application structure using **Flask** for backend functionalities and the **MERN stack** for frontend development, providing an intuitive platform for disease prediction.']}, {'name': 'Flight Price Predictor', 'points': [\"Enhanced the predictive model using **Scikit-learn's regression techniques** and **LSTM networks** for dynamic price forecasting, incorporating temporal data analysis.\", 'Conducted extensive **feature engineering** with **Pandas**, improving model accuracy by identifying key price determinants from historical flight data.', 'Deployed the enhanced model on **Heroku** with a **Flask** web application interface, providing users with real-time, accurate flight price predictions.']}, {'name': 'Generating Synthetic Anime Data', 'points': ['Leveraged **PyTorch** and a **DC GAN architecture** to create high-quality synthetic anime images, significantly enhancing the diversity of machine learning datasets.', 'Utilized **OpenCV** for advanced image preprocessing and augmentation tasks, including resizing, normalization, and color adjustments.', 'Developed automated **Python scripts** for dataset creation, streamlining the collection and preprocessing of anime images to support large-scale training data generation.']}]}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m-----------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_53492\\563257271.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_resume_docx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimized_content\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemplate_style\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_53492\\563574065.py\u001b[0m in \u001b[0;36mgenerate_resume_docx\u001b[1;34m(cv_data, optimized_content, template_style)\u001b[0m\n\u001b[0;32m    185\u001b[0m             \u001b[1;31m# Company and title in bold\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_paragraph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m             \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"name\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m             \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparagraph_format\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspace_after\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "doc = generate_resume_docx(cv_data, optimized_content, template_style)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "8fb699b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_resume_prompts(cv_data: Dict, job_description: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Create prompts for resume sections including company name extraction\n",
    "    \"\"\"\n",
    "    prompts = {\n",
    "        \"company_extraction\": \"\"\"\n",
    "        Extract the exact company name from this job description.\n",
    "        If no company name is found, return 'Company'.\n",
    "        Return only the company name without any additional text.\n",
    "        \n",
    "        Job Description: {job_description}\n",
    "        \"\"\",\n",
    "        \n",
    "        \"skills\": f\"\"\"\n",
    "        Given these skills and job description, return the most relevant ones maintaining the same categories.\n",
    "        Each line should be 10-16 words maximum.\n",
    "        \n",
    "        Current Skills: {cv_data['Skills']}\n",
    "        Job Description: {job_description}\n",
    "        \n",
    "        Format as JSON:\n",
    "        {{\n",
    "            \"Programming Languages\": \"...\",\n",
    "            \"Frameworks\": \"...\",\n",
    "            \"Databases\": \"...\",\n",
    "            \"Technologies\": \"...\",\n",
    "            \"ML Algorithms/Techniques\": \"...\"\n",
    "        }}\n",
    "        \"\"\",\n",
    "        \n",
    "        \"experience\": f\"\"\"\n",
    "        Analyze the job description first to identify the most relevant experience. Then optimize the experiences using STAR method.\n",
    "        \n",
    "        Rules:\n",
    "        1. Format each experience exactly as: \"Company Name, Location - Title    Date\"\n",
    "        2. Points distribution:\n",
    "           - Most relevant experience: 4 points\n",
    "           - Moderately relevant: 3 points\n",
    "           - Less relevant: 2 points\n",
    "        3. Each bullet point must be 12-20 words and include:\n",
    "           - Action verb\n",
    "           - Technical tools/skills/frameworks used (bold these)\n",
    "           - Quantifiable impact/metrics\n",
    "        \n",
    "        Current Experience: {cv_data['Professional Experience And Internships']}\n",
    "        Job Description: {job_description}\n",
    "        \n",
    "        Return as JSON:\n",
    "        {{\n",
    "            \"experiences\": [\n",
    "                {{\n",
    "                    \"full_title\": \"Company Name, Location - Title    Date\",\n",
    "                    \"relevance_score\": \"high/medium/low\",\n",
    "                    \"points\": [\"point1\", \"point2\", ...]\n",
    "                }}\n",
    "            ]\n",
    "        }}\n",
    "        \"\"\",\n",
    "        \n",
    "        \"projects\": f\"\"\"\n",
    "        Select and optimize 2 most relevant projects based on the job description requirements.\n",
    "        \n",
    "        Rules:\n",
    "        1. Select projects that best demonstrate skills mentioned in job description\n",
    "        2. 2-3 bullet points per project (10-20 words each)\n",
    "        3. Each point must include:\n",
    "           - Technical tools/frameworks used (bold these)\n",
    "           - Implementation details\n",
    "           - Measurable outcomes\n",
    "        \n",
    "        Current Projects: {cv_data['Projects']}\n",
    "        Job Description: {job_description}\n",
    "        \n",
    "        Return as JSON:\n",
    "        {{\n",
    "            \"projects\": [\n",
    "                {{\n",
    "                    \"name\": \"Project Name\",\n",
    "                    \"points\": [\n",
    "                        \"Used **Tool/Framework** to implement... resulting in X% improvement\"\n",
    "                    ]\n",
    "                }}\n",
    "            ]\n",
    "        }}\n",
    "        \"\"\"\n",
    "    }\n",
    "    return prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "030594e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate_resume_docx(cv_data: Dict, optimized_content: Dict, template_style: Dict) -> Document:\n",
    "    \"\"\"\n",
    "    Generate resume DOCX with improved formatting\n",
    "    \"\"\"\n",
    "    doc = Document()\n",
    "    \n",
    "    # Set margins\n",
    "    for section in doc.sections:\n",
    "        section.top_margin = Inches(0.3)  # Reduced margins\n",
    "        section.bottom_margin = Inches(0.3)\n",
    "        section.left_margin = Inches(0.5)\n",
    "        section.right_margin = Inches(0.5)\n",
    "\n",
    "    # Name\n",
    "    name_paragraph = doc.add_paragraph()\n",
    "    name_run = name_paragraph.add_run(cv_data[\"Name\"])\n",
    "    name_run.font.size = Pt(16)\n",
    "    name_run.font.bold = True\n",
    "    name_paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "\n",
    "    # Contact Info with hyperlinks\n",
    "    contact_paragraph = doc.add_paragraph()\n",
    "    contact_paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "    \n",
    "    # Add email and phone\n",
    "    contact_run = contact_paragraph.add_run(f\"{cv_data['Contact']['email']} | {cv_data['Contact']['phone']} | \")\n",
    "    contact_run.font.size = Pt(10.5)\n",
    "    \n",
    "    # Add hyperlinks\n",
    "    for platform, url in cv_data[\"Contact\"][\"links\"].items():\n",
    "#         if url:\n",
    "#             hyperlink = contact_paragraph.add_hyperlink(url, platform)\n",
    "#             hyperlink.font.size = Pt(10.5)\n",
    "#             if platform != list(cv_data[\"Contact\"][\"links\"].keys())[-1]:\n",
    "#                 contact_paragraph.add_run(\" | \")\n",
    "        if url:\n",
    "            add_hyperlink(contact_paragraph, url, platform)  # Use the helper function\n",
    "            if platform != list(cv_data[\"Contact\"][\"links\"].keys())[-1]:\n",
    "                contact_paragraph.add_run(\" | \")\n",
    "\n",
    "#     def add_section_heading(title):\n",
    "#         \"\"\"Add section heading with blue color and separator line\"\"\"\n",
    "#         paragraph = doc.add_paragraph()\n",
    "#         paragraph.space_before = Pt(6)  # Reduced spacing\n",
    "#         paragraph.space_after = Pt(3)\n",
    "#         run = paragraph.add_run(title)\n",
    "#         run.font.size = Pt(11.5)\n",
    "#         run.font.bold = True\n",
    "#         run.font.color.rgb = RGBColor(0, 0, 139)  # Dark blue\n",
    "        \n",
    "#         # Add separator line\n",
    "#         border = paragraph.paragraph_format.border\n",
    "#         border.bottom_style = WD_BORDER.SINGLE\n",
    "#         border.bottom_color.rgb = RGBColor(0, 0, 139)\n",
    "    def add_section_heading(title):\n",
    "        \"\"\"Add section heading with blue color and separator line\"\"\"\n",
    "        paragraph = doc.add_paragraph()\n",
    "        paragraph.space_before = Pt(0)  # Further reduced spacing\n",
    "        paragraph.space_after = Pt(0)\n",
    "        run = paragraph.add_run(title)\n",
    "        run.font.size = Pt(11.5)\n",
    "        run.font.bold = True\n",
    "        run.font.color.rgb = RGBColor(0, 0, 139)\n",
    "\n",
    "        # Add separator line\n",
    "        separator = doc.add_paragraph()\n",
    "        separator_format = separator.paragraph_format\n",
    "        separator_format.space_before = Pt(0)\n",
    "        separator_format.space_after = Pt(0)\n",
    "        separator_run = separator.add_run(\"_\" * 150)\n",
    "        separator_run.font.color.rgb = RGBColor(0, 0, 139)\n",
    "        separator_run.font.size = Pt(8)\n",
    "\n",
    "    # Education section with aligned dates\n",
    "    add_section_heading(\"EDUCATION\")\n",
    "    for edu in cv_data[\"Education\"]:\n",
    "        p = doc.add_paragraph()\n",
    "        if \"Coursework:\" in edu:\n",
    "            # Split into main education info and coursework\n",
    "            main_part, coursework_part = edu.split(\"Coursework:\", 1)\n",
    "\n",
    "            # Handle main part (institution, degree, date)\n",
    "            if \"GPA:\" in main_part:\n",
    "                info, gpa = main_part.rsplit(\"GPA:\", 1)\n",
    "                p.add_run(info).bold = True\n",
    "                p.add_run(\"GPA:\").bold = False\n",
    "                p.add_run(gpa.strip()).bold = False\n",
    "            else:\n",
    "                p.add_run(main_part).bold = True\n",
    "\n",
    "            # Add coursework (unbolded)\n",
    "            p.add_run(\"Coursework:\").bold = False\n",
    "            p.add_run(coursework_part.strip()).bold = False\n",
    "        else:\n",
    "            # If no coursework, just handle the main part\n",
    "            if \"GPA:\" in edu:\n",
    "                info, gpa = edu.rsplit(\"GPA:\", 1)\n",
    "                p.add_run(info).bold = True\n",
    "                p.add_run(\"GPA:\").bold = False\n",
    "                p.add_run(gpa.strip()).bold = False\n",
    "            else:\n",
    "                p.add_run(edu).bold = True\n",
    "\n",
    "        \n",
    "    p.paragraph_format.space_after = Pt(0)\n",
    "    # Skills\n",
    "    add_section_heading(\"SKILLS\")\n",
    "    skills_data = parse_json_content(optimized_content[\"skills\"])\n",
    "    if skills_data:\n",
    "        for category, skills in skills_data.items():\n",
    "            if skills:\n",
    "                p = doc.add_paragraph()\n",
    "                p.add_run(f\"{category}: \").bold = True\n",
    "                p.add_run(skills)\n",
    "                p.paragraph_format.space_after = Pt(1)\n",
    "\n",
    "# Experience section with proper formatting\n",
    "    add_section_heading(\"PROFESSIONAL EXPERIENCE AND INTERNSHIPS\")\n",
    "    experience_data = parse_json_content(optimized_content[\"experience\"])\n",
    "    if experience_data and 'experiences' in experience_data:\n",
    "        for exp in experience_data['experiences']:\n",
    "            p = doc.add_paragraph()\n",
    "            p.add_run(exp['full_title']).bold = True\n",
    "            p.paragraph_format.space_after = Pt(1)\n",
    "            \n",
    "            # Add points based on relevance score\n",
    "            max_points = 4 if exp['relevance_score'] == 'high' else (2 if exp['relevance_score'] == 'medium' else 2)\n",
    "            for point in exp['points'][:max_points]:\n",
    "                bullet_p = doc.add_paragraph(style='List Bullet')\n",
    "                bullet_p.paragraph_format.left_indent = Inches(0.25)\n",
    "                bullet_p.paragraph_format.space_after = Pt(1)\n",
    "#                 bullet_p.add_run(point)\n",
    "                # Handle bold text in points\n",
    "                parts = point.split('**')\n",
    "                for i, part in enumerate(parts):\n",
    "                    run = bullet_p.add_run(part)\n",
    "                    if i % 2 == 1:  # Bold text between **\n",
    "                        run.bold = True\n",
    "\n",
    "    # Projects section\n",
    "    add_section_heading(\"PROJECTS\")\n",
    "    projects_data = parse_json_content(optimized_content[\"projects\"])\n",
    "    if projects_data and 'projects' in projects_data:\n",
    "        for project in projects_data['projects']:\n",
    "            p = doc.add_paragraph()\n",
    "            p.add_run(project['name']).bold = True\n",
    "            p.paragraph_format.space_after = Pt(1)\n",
    "            \n",
    "            for point in project['points']:\n",
    "                bullet_p = doc.add_paragraph(style='List Bullet')\n",
    "                bullet_p.paragraph_format.left_indent = Inches(0.25)\n",
    "                bullet_p.paragraph_format.space_after = Pt(1)\n",
    "                # Handle bold text in points\n",
    "                parts = point.split('**')\n",
    "                for i, part in enumerate(parts):\n",
    "                    run = bullet_p.add_run(part)\n",
    "                    if i % 2 == 1:  # Bold text between **\n",
    "                        run.bold = True\n",
    "\n",
    "    return doc\n",
    "def process_complete_resume(cv_data: Dict, job_description: str, client: OpenAI) -> Tuple[Optional[str], Optional[str]]:\n",
    "    \"\"\"\n",
    "    Complete resume generation process with error handling\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get prompts\n",
    "        prompts = create_resume_prompts(cv_data, job_description)\n",
    "        \n",
    "        # Extract company name\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo-16k\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a job description analyzer.\"},\n",
    "                {\"role\": \"user\", \"content\": prompts[\"company_extraction\"].format(job_description=job_description)}\n",
    "            ]\n",
    "        )\n",
    "        company_name = response.choices[0].message.content.strip()\n",
    "        \n",
    "        # Get optimized content\n",
    "        optimized_content = {}\n",
    "        for section in [\"skills\", \"experience\",\"projects\"]:\n",
    "            response = client.chat.completions.create(\n",
    "#                 model=\"gpt-3.5-turbo-16k\",\n",
    "                  model=\"o1-mini-2024-09-12\",\n",
    "\n",
    "                messages=[\n",
    "                    {\"role\": \"assistant\", \"content\": \"You are an expert resume writer.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompts[section]}\n",
    "                ],\n",
    "                max_completion_tokens=10000,\n",
    "#                 reasoning_effort=\"high\"\n",
    "\n",
    "                \n",
    "            )\n",
    "            optimized_content[section] = response.choices[0].message.content\n",
    "        \n",
    "        # Template styling\n",
    "        template_style = {\n",
    "            \"name_size\": 16,\n",
    "            \"contact_size\": 10.5,\n",
    "            \"heading_size\": 11.5,\n",
    "            \"content_size\": 10\n",
    "        }\n",
    "        print(\"Optimized Content\")\n",
    "        print(optimized_content)\n",
    "        print(\"-----------\")      \n",
    "        return optimized_content # Generate document\n",
    "#         doc = generate_resume_docx(cv_data, optimized_content, template_style)\n",
    "#         if not doc:\n",
    "#             logger.error(\"Failed to generate resume document\")\n",
    "#             return None, None\n",
    "        \n",
    "#         # Save both versions\n",
    "#         try:\n",
    "#             docx_path, pdf_path = save_resume(doc, company_name, cv_data)\n",
    "#             return docx_path, pdf_path\n",
    "#         except Exception as e:\n",
    "#             logger.error(f\"Error saving resume: {e}\")\n",
    "#             return None, None\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in resume generation process: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d3d1f4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linkedin  linkedin https://www.linkedin.com/in/mohammad-faseeh-ahmed/\n",
      "https://www.linkedin.com/in/mohammad-faseeh-ahmed/\n",
      "github github https://github.com/faseehahmed26\n",
      "https://github.com/faseehahmed26\n",
      "portfolio portfolio https://faseehahmed26.github.io/portfolio/\n",
      "https://faseehahmed26.github.io/portfolio/\n",
      "kaggle kaggle https://www.kaggle.com/faseeh001\n",
      "https://www.kaggle.com/faseeh001\n",
      "tableau tableau https://public.tableau.com/app/profile/faseeh5112\n",
      "https://public.tableau.com/app/profile/faseeh5112\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Name': 'Mohammad Faseeh Ahmed',\n",
       " 'Contact': {'email': 'mm9314@g.rit.edu',\n",
       "  'phone': '+1 585 202 5217',\n",
       "  'links': {'LinkedIn': 'https://www.linkedin.com/in/mohammad-faseeh-ahmed/',\n",
       "   'Github': 'https://github.com/faseehahmed26',\n",
       "   'Portfolio': 'https://faseehahmed26.github.io/portfolio/',\n",
       "   'Kaggle': 'https://www.kaggle.com/faseeh001',\n",
       "   'Tableau': 'https://public.tableau.com/app/profile/faseeh5112'}},\n",
       " 'Education': ['Rochester Institute of Technology, Rochester, NY,  M.S in Data Science\\t      Expected May 2025',\n",
       "  'Coursework: Neural Networks, Software Engineering for Data Science, Applied Statistics.                         GPA: 3.84/4.00',\n",
       "  'Jawaharlal Nehru Technological University Hyderabad, B.Tech in Computer Science\\t July 2018 - July 2022',\n",
       "  'Coursework: Data Structures and Algorithms, Computer Vision, Artificial Intelligence, NLP \\t         GPA: 3.2/4.00'],\n",
       " 'Skills': {'Programming Languages': 'Java, Python, C++, R, JavaScript, Object Oriented Programming(Python, Java)',\n",
       "  'Frameworks': 'PyTorch, Keras, Scikit, Tensorflow, Groovy, PySpark, Flask, React, NodeJS,',\n",
       "  'Databases': 'SQL, MongoDB, SQLite, MySQL, NoSQL, PostgreSQL, DynamoDB',\n",
       "  'Technologies': '',\n",
       "  'ML Algorithms/Techniques': 'Regression, Classification, Clustering, Recommender Systems, Deep Learning, NLP, A/B Testing, Time Series, Optimization, EDA, ETL,'},\n",
       " 'Professional Experience And Internships': [{'title': 'Daiichi Sankyo Inc, Basking Ridge, NJ- R&D Data Governance Intern\\t        03/2024 - Present',\n",
       "   'points': ['Developed an Informed Consent Forms (ICF) analysis tool using BERT and T5 models, also testing Amazon Bedrock for large-scale language processing.',\n",
       "    'Built a Flask-based frontend for the ICF tool, enabling secure modifications of document classifications based on user permissions.',\n",
       "    'Implemented advanced Large Language Models (LLMs) using Amazon Bedrock, achieving a 20% increase in classification accuracy for detecting explicit prohibitions against data sharing.',\n",
       "    'Utilized Amazon SageMaker for model training and fine-tuning, streamlining the handling of vast volumes of legal documents to ensure scalability and efficiency.',\n",
       "    'Optimized model inference and processing times with EC2 instances, improving overall performance and reliability of the ICF analysis tool.',\n",
       "    '1. Designed an on-premise chatbot delivering instant trial data, cutting access time by 70%.',\n",
       "    '2. Enabled visualizations in the chatbot, improving data comprehension and decision-making efficiency.',\n",
       "    '3. Automated milestone and site tracking, enhancing study performance and reducing missed deadlines by 30%.',\n",
       "    '4. Developed a secure chatbot architecture, ensuring 100% compliance with company data regulations.',\n",
       "    '5. Integrated RAG and AI, achieving 80% improvement in answer accuracy and user trust.',\n",
       "    'Led the migration of clinical data from Veeva Vault to Redshift, establishing an ETL pipeline that ensured data accuracy and improved analytics accessibility.',\n",
       "    'Built indexing and foreign key relationships in Redshift to optimize query performance, enabling high-speed analysis on clinical data.',\n",
       "    'Implemented data validation checks during migration to uphold data governance and regulatory compliance, ensuring reliable and accurate data in the new environment.',\n",
       "    \"Collaborating with stakeholders to align the tool's capabilities with Daiichi Sankyo's data-sharing policies, compliance standards, and real-world data requirements.\",\n",
       "    'Rochester Institute Of Technology, Rochester, NY- Research Assistant\\t        08/2024 - Present',\n",
       "    'Collaborating with Professor Haibo Yang to enhance federated learning models, utilizing gRPC and PyTorch to implement scalable decentralized training algorithms.',\n",
       "    'Analyzing and optimizing distributed environments with PyTorch RPC, reducing communication overhead by 15% through improved data transfer protocols.',\n",
       "    'Leading the integration of the FedDisco algorithm into existing frameworks, accelerating model convergence and reducing training time by 20%.',\n",
       "    \"Transitioning between distributed communication algorithms, leveraging FedDisco's adaptive communication strategies for efficient, real-time model updates.\",\n",
       "    'Developing advanced distributed training techniques for Large Language Models (LLMs), achieving a 30% increase in system performance and minimizing network latency.',\n",
       "    'Utilizing gRPC for dynamic, secure communication within federated learning models, enhancing data synchronization and increasing overall model accuracy by 10%.',\n",
       "    'Designing hands-on projects for the Intro to Machine Learning course, applying theory to real-world machine learning scenarios using Python and PyTorch.',\n",
       "    'Providing personalized support to over 25 students, fostering problem-solving skills and guiding successful project outcomes, significantly improving their understanding of distributed machine learning concepts.']},\n",
       "  {'title': 'SEO Content AI, Los Angeles, CA - AI Infrastructure Engineer  \\t Nov 2022 - July 2023',\n",
       "   'points': ['Enhanced AI-driven content generation efficiency by 25% through leading the integration of transformers within AWS microservices.',\n",
       "    'Developed a Chrome extension using Python, JavaScript, and NodeJS, boosting content quality and generation speed by 40%.',\n",
       "    'Utilized Docker for scalable and reliable deployment across cloud environments via AWS ECS and Fargate.',\n",
       "    'Improved project management and team collaboration through implementing effective development methodologies.',\n",
       "    'Optimized content creation for various use cases by conducting A/B testing with models like LLaMA-13B, Flan-T5, Vicuna-13B, and GPT-3.5.',\n",
       "    \"Broadened audience reach by using GCP's Google Translate API for multi-language content translation.\",\n",
       "    'Ensured high-quality, error-free content by implementing NLP algorithms with BERT, RoBERTa, and DistilBERT for grammar correction, significantly enhancing user trust.']},\n",
       "  {'title': 'White Label Resell, Los Angeles, CA - Machine Learning Engineer                                      June 2022 - March 2023',\n",
       "   'points': ['Automated article generation for digital marketing using AWS Lambda, NodeJS, and API Gateway achieving a 60x reduction in operational costs.',\n",
       "    'Integrated NLP and TensorFlow to analyze and generate over 130K high-quality articles within a week, improving content strategy.',\n",
       "    'Utilized DynamoDB for managing large datasets, ensuring efficient data access and manipulation for content generation processes.',\n",
       "    'Fine-tuned advanced NLP models like BERT, RoBERTa, ALBERT, and DistilBERT for contextual understanding, improving the relevance and quality of generated articles.',\n",
       "    'Explored generative models including GPT-3, GPT-Neo, GPT-J, and GPT-NeoX, to diversify content creation, ensuring a wide range of engaging and unique articles.',\n",
       "    'Leveraged Flan-T5 and T5 Codegen for specific content generation tasks, optimizing for both efficiency and creativity in article production.',\n",
       "    'Developed custom dataset models with Gensim, enhancing thematic accuracy and alignment with marketing goals.',\n",
       "    'Employed MLOps practices with Git and Docker to streamline the development, training, and deployment of machine learning models, facilitating continuous improvement and collaboration.']},\n",
       "  {'title': 'Digital Clinics Research and Services, Hyderabad, India - Data Scientist Intern\\tNov 2021 - Dec 2022',\n",
       "   'points': ['Developed a sophisticated image classification system using Faster R-CNN, tailored for the nuanced detection of cancerous cells in histopathological images, leveraging TensorFlow for model training and optimization.']},\n",
       "  {'title': 'Engineered a custom segmentation solution with Detectron2, enabling precise delineation of tumor boundaries in medical scans. This approach utilized QuPath for image management and Groovy for scripting complex analysis workflows, significantly enhancing the quality of medical image analysis.',\n",
       "   'points': ['Implemented YOLOv5 for the automated screening of pathological slides, achieving unprecedented speed and accuracy in identifying critical diagnostic markers, integrated with OpenCV for real-time image processing and enhancements.',\n",
       "    'Led the integration of MLOps practices, utilizing Docker for containerization and AWS for model deployment, ensuring scalable and robust AI solutions in clinical settings.',\n",
       "    \"Mentored a diverse group of 10-20 junior colleagues (first and second-year students) on multiple ML and DL projects, fostering a collaborative learning environment and enhancing the team's overall AI expertise and project execution capabilities.\"]},\n",
       "  {'title': 'Edgeforce Solutions, Hyderabad, India - Data Scientist Intern\\tNov 2021 - Feb 2022',\n",
       "   'points': ['Built a YOLOv5-based real-time object detection system using Python and TensorFlow, integrated with Streamlit for a user-friendly interface, achieving 90% accuracy.']},\n",
       "  {'title': 'Engineered a deep learning model for speech recognition with PyTorch, achieving 95% accuracy, deployed in an Army Walkie Talkie emulator.',\n",
       "   'points': ['Utilized Docker and AWS for deploying scalable and efficient machine learning models, reducing operational costs and improving resource utilization.',\n",
       "    'Leveraged SQL and SQLite for efficient data storage and retrieval, enhancing system performance and data management.',\n",
       "    \"Conducted A/B testing to optimize the model's performance, resulting in enhanced accuracy and user satisfaction.\"]}],\n",
       " 'Projects': [{'name': 'Chronic Kidney Disease Predictor \\t Nov 2022 - July 20',\n",
       "   'points': []},\n",
       "  {'name': \"Utilized Python's Pandas and Numpy for data manipulation and preprocessing, ensuring accurate model inputs.\",\n",
       "   'points': []},\n",
       "  {'name': 'Implemented Scikit-learn to develop a logistic regression model, achieving a 98% F1 score for kidney disease prediction.',\n",
       "   'points': []},\n",
       "  {'name': 'Orchestrated a hybrid application structure, employing Flask for backend functionalities and leveraging the MERN stack for frontend development, to provide an intuitive platform for disease prediction.',\n",
       "   'points': []},\n",
       "  {'name': 'Employed Matplotlib and Seaborn for data visualization, providing insightful analytics for model performance evaluation.',\n",
       "   'points': []},\n",
       "  {'name': 'Applied MongoDB for efficient data storage, enabling scalable and secure management of patient data.',\n",
       "   'points': []},\n",
       "  {'name': 'Beverage Management System                                                                                        June 2022 - March 2023',\n",
       "   'points': []},\n",
       "  {'name': 'Implemented TensorFlow for creating a convolutional neural network model, achieving real-time beverage detection with 98% accuracy.',\n",
       "   'points': []},\n",
       "  {'name': \"Used OpenCV for image processing, enhancing the model's ability to recognize various beverages under different conditions.\",\n",
       "   'points': []},\n",
       "  {'name': 'Developed a web application using Flask as the backend and ReactJS for the frontend, offering a seamless user experience.',\n",
       "   'points': []},\n",
       "  {'name': 'Leveraged PostgreSQL for database management, ensuring robust and reliable storage of inventory data.',\n",
       "   'points': []},\n",
       "  {'name': 'Incorporated Docker for deploying the application, ensuring consistency across development and production environments.',\n",
       "   'points': []},\n",
       "  {'name': 'Covid19 Bot  \\t Nov 2022 - July 20', 'points': []},\n",
       "  {'name': 'Developed a chatbot using Python, Flask, and DialogFlow to offer real-time COVID-19 updates via Telegram, enhancing accessibility for users.',\n",
       "   'points': []},\n",
       "  {'name': 'Implemented Pandas and Numpy for robust data analysis and numerical computations to provide accurate and up-to-date health statistics.',\n",
       "   'points': []},\n",
       "  {'name': 'Integrated MongoDB to efficiently store and manage user queries and responses, facilitating personalized interaction and data retrieval.',\n",
       "   'points': []},\n",
       "  {'name': \"Utilized RapidAPI for accessing live COVID-19 data, ensuring the chatbot's information was current and reliable for users seeking instant updates.\",\n",
       "   'points': []},\n",
       "  {'name': 'Flight Price Predictor  \\t Nov 2022 - July 20', 'points': []},\n",
       "  {'name': \"Enhanced the predictive model using Scikit-learn's regression techniques and LSTM networks for dynamic price forecasting, incorporating temporal data analysis.\",\n",
       "   'points': []},\n",
       "  {'name': 'Conducted extensive feature engineering with Pandas, improving model accuracy by identifying key price determinants from historical flight data.',\n",
       "   'points': []},\n",
       "  {'name': 'Utilized Matplotlib and Seaborn for insightful visualizations of data trends and model performance, aiding in the interpretability of predictions.',\n",
       "   'points': []},\n",
       "  {'name': 'Deployed the enhanced model on Heroku with a Flask web application interface, providing users with real-time, accurate flight price predictions.',\n",
       "   'points': []},\n",
       "  {'name': 'Generating Synthetic Anime Data                                                                                        June 2022 - March 2023',\n",
       "   'points': []},\n",
       "  {'name': 'Leveraged PyTorch and a DC GAN architecture to create high-quality synthetic anime images, significantly enhancing the diversity of our machine learning datasets.',\n",
       "   'points': []},\n",
       "  {'name': \"Utilized OpenCV for advanced image preprocessing and augmentation tasks, including resizing, normalization, and color adjustments, to prepare images for training, enhancing the model's ability to generate diverse and realistic anime characters.\",\n",
       "   'points': []},\n",
       "  {'name': 'Developed Python scripts to automate the collection and preprocessing of anime images, streamlining the dataset creation process and enabling the efficient generation of large volumes of training data.',\n",
       "   'points': []},\n",
       "  {'name': \"Applied OpenCV's edge detection and image filtering capabilities to enhance the detail and quality of generated images, ensuring a high degree of realism and variability in the synthetic data.\",\n",
       "   'points': []},\n",
       "  {'name': 'Integrated additional data augmentation techniques such as flipping, rotation, and scaling through OpenCV, further augmenting the training dataset to improve the robustness and generalization of the GAN model.',\n",
       "   'points': []},\n",
       "  {'name': 'Number Plate Detection  \\t Nov 2022 - July 20', 'points': []},\n",
       "  {'name': 'Deployed YOLOv5 for real-time number plate detection, achieving high accuracy in diverse lighting and environmental conditions.',\n",
       "   'points': []},\n",
       "  {'name': 'Incorporated OpenCV for advanced image processing tasks, such as adaptive thresholding and contour detection, to improve the accuracy of number plate recognition.',\n",
       "   'points': []},\n",
       "  {'name': 'Developed a system using Flask as the backend, enabling real-time detection and information processing, with a user-friendly interface for monitoring and alerts.',\n",
       "   'points': []},\n",
       "  {'name': \"Integrated OpenCV's Gaussian Blur to reduce image noise and improve the model's detection capabilities in varying environmental conditions.\",\n",
       "   'points': []},\n",
       "  {'name': \"Implemented perspective transformation techniques in OpenCV to correct angles and enhance the readability of number plates, ensuring accurate recognition regardless of the vehicle's orientation.\",\n",
       "   'points': []},\n",
       "  {'name': 'Research Papers', 'points': []},\n",
       "  {'name': 'Tubules Detection on Breast Carcinoma Whole Slide Images Using Artificial Intelligence (Sep 2022):',\n",
       "   'points': []},\n",
       "  {'name': \"This groundbreaking research utilized advanced AI deep learning models to accurately detect tubules in breast carcinoma images. The abstract was recognized for its innovation and selected for presentation at the prestigious International C-MIMI Conference at Johns Hopkins University, underscoring the project's contribution to medical imaging and diagnostics.\",\n",
       "   'points': []},\n",
       "  {'name': 'Web-Based Mitosis Detection on Breast Cancer Whole Slide Images Using FasterRCNN and YOLOv5 (Dec 2022):',\n",
       "   'points': []},\n",
       "  {'name': 'This significant paper, published by the Science and Information (SAI) Organization in the International Journal of Advanced Computer Science Applications (IJACA), detailed the development and application of Faster R-CNN and YOLOv5 for the detection of mitosis in breast cancer whole slide images. The research provided insights into the capabilities of these models in enhancing the accuracy and efficiency of breast cancer grading, demonstrating a pivotal step forward in the application of machine learning in healthcare.',\n",
       "   'points': []},\n",
       "  {'name': 'Kaggle Notebook and Dataset Expert', 'points': []},\n",
       "  {'name': 'Open Source Contributor at OpenVino(Intel), Ivy', 'points': []},\n",
       "  {'name': \"Won Best Overall Hack in CSH Hacks Hackathon 2023 : Developed a Chrome extension with a streamlined front-end, leveraging AWS Lambda for backend deployment, which integrates OpenAI's GPT-3.5 to scrutinize terms and conditions, enhancing user vigilance through AWS-powered processing and API Gateway for efficient data handling, encapsulating cloud architecture expertise in delivering a user-centric solution.\",\n",
       "   'points': []}],\n",
       " 'Research Papers': []}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_data = extract_cv_data(doc)  # Using the extract_cv_data function from earlier\n",
    "cv_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "1da2334c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_cv_data(doc: Document) -> Dict:\n",
    "    \"\"\"\n",
    "    Extract and structure CV data from document\n",
    "    \"\"\"\n",
    "    cv_data = {\n",
    "        \"Name\": \"\",\n",
    "        \"Contact\": {\n",
    "            'email': '',\n",
    "            'phone': '',\n",
    "            'links':{'LinkedIn': '',\n",
    "            'Github': '',\n",
    "            'Portfolio': '',\n",
    "            'Kaggle': '',\n",
    "            'Tableau': ''}\n",
    "        },\n",
    "        \"Education\": [],\n",
    "        \"Skills\": {\n",
    "            \"Programming Languages\": \"\",\n",
    "            \"Frameworks\": \"\",\n",
    "            \"Databases\": \"\",\n",
    "            \"Technologies\": \"\",\n",
    "            \"ML Algorithms/Techniques\": \"\"\n",
    "        },\n",
    "        \"Professional Experience And Internships\": [],\n",
    "        \"Projects\": [],\n",
    "        \"Research Papers\": []\n",
    "    }\n",
    "    \n",
    "    # Initialize current section\n",
    "    current_section = None\n",
    "    \n",
    "    for idx, para in enumerate(doc.paragraphs):\n",
    "        text = para.text.strip()\n",
    "        if not text:\n",
    "            continue\n",
    "\n",
    "        # Track sections\n",
    "        if \"EDUCATION\" in text:\n",
    "            current_section = \"education\"\n",
    "            continue\n",
    "        elif \"SKILLS\" in text:\n",
    "            current_section = \"skills\"\n",
    "            continue\n",
    "        elif \"PROFESSIONAL EXPERIENCE\" in text:\n",
    "            current_section = \"experience\"\n",
    "            continue\n",
    "        elif \"PROJECTS\" in text:\n",
    "            current_section = \"projects\"\n",
    "            continue\n",
    "        elif \"RESEARCH PAPERS\" in text:\n",
    "            current_section = \"research\"\n",
    "            continue\n",
    "\n",
    "        # Extract Name\n",
    "        if idx == 0:\n",
    "            cv_data[\"Name\"] = text\n",
    "            continue\n",
    "\n",
    "        # Extract Contact Info and Links\n",
    "        if idx == 1:\n",
    "            parts = text.split(' | ')\n",
    "            cv_data[\"Contact\"][\"email\"] = parts[0]\n",
    "            cv_data[\"Contact\"][\"phone\"] = parts[1]\n",
    "#                         elif any(x in hyperlink.text.lower() for x in ['linkedin', 'github', 'portfolio', 'kaggle', 'tableau']):\n",
    "#                 static_content['contact_info']['links'][hyperlink.text.lower()] = hyperlink.address\n",
    "            # Extract hyperlinks\n",
    "            if para.hyperlinks:\n",
    "                for hyperlink in para.hyperlinks:\n",
    "                    for platform in ['LinkedIn', 'Github', 'Portfolio', 'Kaggle', 'Tableau']:\n",
    "#                         print(hyperlink.text.lower(),platform.lower(), hyperlink.address)\n",
    "#                         print(str(platform.lower()) in str(hyperlink.text.lower()).strip())\n",
    "                        if str(platform.lower()) in  str(hyperlink.text.lower()).strip():\n",
    "                            print(hyperlink.text.lower(),platform.lower(), hyperlink.address)\n",
    "                            cv_data[\"Contact\"]['links'][platform] = hyperlink.address\n",
    "                            print(cv_data[\"Contact\"]['links'][platform])\n",
    "            continue\n",
    "\n",
    "        # Process sections\n",
    "        if current_section == \"education\":\n",
    "            if any(x in text for x in [\"Institute\", \"University\", \"GPA\"]):\n",
    "                cv_data[\"Education\"].append(text)\n",
    "                \n",
    "        elif current_section == \"skills\":\n",
    "            if text.startswith(\"Programming Languages:\"):\n",
    "                cv_data[\"Skills\"][\"Programming Languages\"] = text.split(\": \")[1]\n",
    "            elif text.startswith(\"Frameworks:\"):\n",
    "                cv_data[\"Skills\"][\"Frameworks\"] = text.split(\": \")[1]\n",
    "            elif text.startswith(\"Databases:\"):\n",
    "                cv_data[\"Skills\"][\"Databases\"] = text.split(\": \")[1]\n",
    "            elif text.startswith(\"Technologies:\"):\n",
    "                cv_data[\"Skills\"][\"Technologies\"] = text.split(\": \")[1]\n",
    "            elif text.startswith(\"ML Algorithms\"):\n",
    "                cv_data[\"Skills\"][\"ML Algorithms/Techniques\"] = text.split(\": \")[1]\n",
    "                \n",
    "        elif current_section == \"experience\":\n",
    "            if any(x in text for x in [\"Intern\", \"Engineer\"]) and not text.startswith(\"•\"):\n",
    "                cv_data[\"Professional Experience And Internships\"].append({\n",
    "                    \"title\": text,\n",
    "                    \"points\": []\n",
    "                })\n",
    "            elif cv_data[\"Professional Experience And Internships\"] and not text.isupper():\n",
    "                cv_data[\"Professional Experience And Internships\"][-1][\"points\"].append(text)\n",
    "                \n",
    "        elif current_section == \"projects\":\n",
    "            if not text.startswith(\"•\") and not text.isupper():\n",
    "                cv_data[\"Projects\"].append({\n",
    "                    \"name\": text,\n",
    "                    \"points\": []\n",
    "                })\n",
    "            elif cv_data[\"Projects\"] and not text.isupper():\n",
    "                cv_data[\"Projects\"][-1][\"points\"].append(text)\n",
    "                \n",
    "        elif current_section == \"research\":\n",
    "            if text.endswith(\":\"):\n",
    "                cv_data[\"Research Papers\"].append({\n",
    "                    \"title\": text,\n",
    "                    \"description\": \"\"\n",
    "                })\n",
    "            elif cv_data[\"Research Papers\"]:\n",
    "                cv_data[\"Research Papers\"][-1][\"description\"] = text\n",
    "\n",
    "    return cv_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "44f81602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_resume_prompts(cv_data: Dict, job_description: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Create prompts for resume sections including company name extraction\n",
    "    \"\"\"\n",
    "    prompts = {\n",
    "        \"company_extraction\": \"\"\"\n",
    "        Extract the exact company name from this job description.\n",
    "        If no company name is found, return 'Company'.\n",
    "        Return only the company name without any additional text.\n",
    "        \n",
    "        Job Description: {job_description}\n",
    "        \"\"\",\n",
    "        \n",
    "        \"skills\": f\"\"\"\n",
    "        Given these skills and job description, return the most relevant ones maintaining the same categories.\n",
    "        Each line should be 10-16 words maximum.\n",
    "        \n",
    "        Current Skills: {cv_data['Skills']}\n",
    "        Job Description: {job_description}\n",
    "        \n",
    "        Format as JSON:\n",
    "        {{\n",
    "            \"Programming Languages\": \"...\",\n",
    "            \"Frameworks\": \"...\",\n",
    "            \"Databases\": \"...\",\n",
    "            \"Technologies\": \"...\",\n",
    "            \"ML Algorithms/Techniques\": \"...\"\n",
    "        }}\n",
    "        \"\"\",\n",
    "        \n",
    "        \"experience\": f\"\"\"\n",
    "        Optimize these experiences for the job description using STAR method.\n",
    "        - Keep company names, titles, and dates exact\n",
    "        - 2-4 bullet points per role (10-16 words each)\n",
    "        - Include technical details and metrics\n",
    "        \n",
    "        Current Experience: {cv_data['Professional Experience And Internships']}\n",
    "        Job Description: {job_description}\n",
    "        \n",
    "        Format as JSON with exact dates and titles.\n",
    "        \"\"\"\n",
    "    }\n",
    "    return prompts\n",
    "\n",
    "def generate_resume_docx(cv_data: Dict, optimized_content: Dict, template_style: Dict) -> Document:\n",
    "    \"\"\"\n",
    "    Generate resume DOCX with specific formatting\n",
    "    \"\"\"\n",
    "    doc = Document()\n",
    "    \n",
    "    # Set margins\n",
    "    for section in doc.sections:\n",
    "        section.top_margin = Inches(0.5)\n",
    "        section.bottom_margin = Inches(0.5)\n",
    "        section.left_margin = Inches(0.5)\n",
    "        section.right_margin = Inches(0.5)\n",
    "\n",
    "    # Name\n",
    "    name_paragraph = doc.add_paragraph()\n",
    "    name_run = name_paragraph.add_run(cv_data[\"Name\"])\n",
    "    name_run.font.size = Pt(16)\n",
    "    name_run.font.bold = True\n",
    "    name_paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "\n",
    "    # Contact Info with hyperlinks\n",
    "    contact_paragraph = doc.add_paragraph()\n",
    "    contact_paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "    \n",
    "    # Add email and phone\n",
    "    contact_run = contact_paragraph.add_run(f\"{cv_data['Contact']['email']} | {cv_data['Contact']['phone']} | \")\n",
    "    contact_run.font.size = Pt(10.5)\n",
    "    \n",
    "    # Add hyperlinked items\n",
    "    for platform, url in cv_data[\"Contact\"][\"links\"].items():\n",
    "        if url:\n",
    "            hyperlink = contact_paragraph.add_run(platform)\n",
    "            hyperlink.font.size = Pt(10.5)\n",
    "            # Add separator except for last item\n",
    "            if platform != list(cv_data[\"Contact\"][\"links\"].keys())[-1]:\n",
    "                contact_paragraph.add_run(\" | \")\n",
    "\n",
    "    # Add sections\n",
    "    def add_section_heading(title):\n",
    "        doc.add_paragraph().add_run().add_break()\n",
    "        heading = doc.add_paragraph()\n",
    "        run = heading.add_run(title)\n",
    "        run.font.size = Pt(11.5)\n",
    "        run.font.bold = True\n",
    "        return heading\n",
    "\n",
    "    # Education\n",
    "    add_section_heading(\"EDUCATION\")\n",
    "    for edu in cv_data[\"Education\"]:\n",
    "        p = doc.add_paragraph()\n",
    "        p.add_run(edu).font.size = Pt(10)\n",
    "\n",
    "    # Skills\n",
    "    add_section_heading(\"SKILLS\")\n",
    "    for category, skills in optimized_content[\"skills\"].items():\n",
    "        p = doc.add_paragraph()\n",
    "        run = p.add_run(f\"{category}: {skills}\")\n",
    "        run.font.size = Pt(10)\n",
    "\n",
    "    # Experience\n",
    "    add_section_heading(\"PROFESSIONAL EXPERIENCE AND INTERNSHIPS\")\n",
    "    for exp in optimized_content[\"experience\"]:\n",
    "        # Add company and title\n",
    "        p = doc.add_paragraph()\n",
    "        run = p.add_run(exp[\"title\"])\n",
    "        run.font.size = Pt(10)\n",
    "        \n",
    "        # Add bullet points\n",
    "        for point in exp[\"points\"]:\n",
    "            p = doc.add_paragraph(point, style='List Bullet')\n",
    "            p.paragraph_format.left_indent = Inches(0.25)\n",
    "            p.runs[0].font.size = Pt(10)\n",
    "\n",
    "    return doc\n",
    "\n",
    "def save_resume(doc: Document, company_name: str, cv_data: Dict, output_dir: str = \"resumes\"):\n",
    "    \"\"\"\n",
    "    Save resume in both DOCX and PDF formats\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Generate base filename\n",
    "    base_name = cv_data[\"Name\"].lower().replace(' ', '_')\n",
    "    company_name = company_name.lower().replace(' ', '_')\n",
    "    base_filename = f\"{base_name}_{company_name}_resume\"\n",
    "    \n",
    "    # Save DOCX\n",
    "    docx_path = os.path.join(output_dir, f\"{base_filename}.docx\")\n",
    "    doc.save(docx_path)\n",
    "    \n",
    "    # Convert to PDF\n",
    "    pdf_path = os.path.join(output_dir, f\"{base_filename}.pdf\")\n",
    "    convert(docx_path, pdf_path)\n",
    "    \n",
    "    return docx_path, pdf_path\n",
    "\n",
    "def process_complete_resume(cv_data: Dict, job_description: str, client: OpenAI):\n",
    "    \"\"\"\n",
    "    Complete resume generation process\n",
    "    \"\"\"\n",
    "    # Get prompts\n",
    "    prompts = create_resume_prompts(cv_data, job_description)\n",
    "    \n",
    "    # Extract company name\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-16k\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a job description analyzer.\"},\n",
    "            {\"role\": \"user\", \"content\": prompts[\"company_extraction\"].format(job_description=job_description)}\n",
    "        ]\n",
    "    )\n",
    "    company_name = response.choices[0].message.content.strip()\n",
    "    \n",
    "    # Get optimized content\n",
    "    optimized_content = {}\n",
    "    for section in [\"skills\", \"experience\"]:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo-16K\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert resume writer.\"},\n",
    "                {\"role\": \"user\", \"content\": prompts[section]}\n",
    "            ],\n",
    "            max_tokens=10000\n",
    "            \n",
    "        )\n",
    "        optimized_content[section] = response.choices[0].message.content\n",
    "    \n",
    "    # Template styling\n",
    "    template_style = {\n",
    "        \"name_size\": 16,\n",
    "        \"contact_size\": 10.5,\n",
    "        \"heading_size\": 11.5,\n",
    "        \"content_size\": 10\n",
    "    }\n",
    "    print(\"Optimized Content\")\n",
    "    print(optimized_content)\n",
    "    print(\"-----------\")\n",
    "    # Generate document\n",
    "    doc = generate_resume_docx(cv_data, optimized_content, template_style)\n",
    "    \n",
    "    # Save both versions\n",
    "    docx_path, pdf_path = save_resume(doc, company_name, cv_data)\n",
    "    \n",
    "    return docx_path, pdf_path\n",
    "\n",
    "# Example usage\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333d0bc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
