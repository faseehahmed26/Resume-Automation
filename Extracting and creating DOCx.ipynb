{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f68b9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_cv(docx_path: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Process a DOCX CV and extract structured information.\n",
    "    Args:\n",
    "        docx_path (str): Path to the DOCX file\n",
    "    Returns:\n",
    "        Dict: Dictionary containing structured CV information\n",
    "    \"\"\"\n",
    "    doc = Document(docx_path)\n",
    "    print(doc.paragraphs.)\n",
    "    cv_data = {}\n",
    "    current_section = None\n",
    "    section_content = []\n",
    "    \n",
    "    # Headers we want to identify (in order they appear in the document)\n",
    "    section_headers = {\n",
    "        'EDUCATION': 'education',\n",
    "        'SKILLS': 'skills',\n",
    "        'PROFESSIONAL EXPERIENCE AND INTERNSHIPS': 'experience',\n",
    "        'PROJECTS': 'projects',\n",
    "        'RESEARCH PAPERS': 'research_papers',\n",
    "        'ACHIEVEMENTS': 'achievements'  # Added achievements section\n",
    "    }\n",
    "\n",
    "    # Extract contact information from the first paragraph\n",
    "    first_para = doc.paragraphs[0].text.strip()\n",
    "    \n",
    "    # Parse contact info\n",
    "    contact_parts = first_para.split('|')\n",
    "    contact_info = {\n",
    "        'email': contact_parts[0].strip(),\n",
    "        'phone': contact_parts[1].strip(),\n",
    "        'linkedin': contact_parts[2].strip(),\n",
    "        'github': contact_parts[3].strip(),\n",
    "        'portfolio': contact_parts[4].strip(),\n",
    "        'kaggle': contact_parts[5].strip(),\n",
    "        'tableau': contact_parts[6].strip() if len(contact_parts) > 6 else None\n",
    "    }\n",
    "    cv_data['contact_info'] = contact_info\n",
    "\n",
    "    # Process each paragraph to extract sections\n",
    "    for para in doc.paragraphs:\n",
    "        text = para.text.strip()\n",
    "        if not text:  # Skip empty paragraphs\n",
    "            continue\n",
    "\n",
    "        # Check if this is a section header\n",
    "        is_header = False\n",
    "        for header, section_name in section_headers.items():\n",
    "            if header in text.upper():\n",
    "                # If we were building a previous section, save it\n",
    "                if current_section and section_content:\n",
    "                    cv_data[current_section] = '\\n'.join(section_content)\n",
    "                # Start new section\n",
    "                current_section = section_name\n",
    "                section_content = []\n",
    "                is_header = True\n",
    "                break\n",
    "\n",
    "        # If not a header and we're in a section, add to content\n",
    "        if not is_header and current_section:\n",
    "            # Skip the section header itself\n",
    "            if text.upper() not in section_headers:\n",
    "                section_content.append(text)\n",
    "\n",
    "    # Add the last section\n",
    "    if current_section and section_content:\n",
    "        cv_data[current_section] = '\\n'.join(section_content)\n",
    "\n",
    "    # Post-process skills section if it wasn't properly captured\n",
    "    if 'skills' not in cv_data or not cv_data['skills'].strip():\n",
    "        # Look for skills section specifically\n",
    "        skills_text = []\n",
    "        found_skills = False\n",
    "        for para in doc.paragraphs:\n",
    "            text = para.text.strip()\n",
    "            if 'SKILLS' in text.upper():\n",
    "                found_skills = True\n",
    "                continue\n",
    "            elif found_skills and any(header in text.upper() for header in section_headers.keys()):\n",
    "                break\n",
    "            elif found_skills and text:\n",
    "                skills_text.append(text)\n",
    "        if skills_text:\n",
    "            cv_data['skills'] = '\\n'.join(skills_text)\n",
    "\n",
    "    # Ensure achievements are captured\n",
    "    if 'ACHIEVEMENTS' not in cv_data:\n",
    "        achievements_text = []\n",
    "        found_achievements = False\n",
    "        for para in doc.paragraphs:\n",
    "            text = para.text.strip()\n",
    "            if 'ACHIEVEMENTS' in text.upper():\n",
    "                found_achievements = True\n",
    "                continue\n",
    "            elif found_achievements and text and not any(header in text.upper() for header in section_headers.keys()):\n",
    "                achievements_text.append(text)\n",
    "        if achievements_text:\n",
    "            cv_data['achievements'] = '\\n'.join(achievements_text)\n",
    "\n",
    "    return cv_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "c4d09237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_all_paragraphs(docx_path: str):\n",
    "    \"\"\"\n",
    "    Print all paragraphs from the DOCX file with their index and text\n",
    "    \"\"\"\n",
    "    doc = Document(docx_path)\n",
    "    \n",
    "    print(\"\\nAll Paragraphs in Document:\")\n",
    "    print(\"=\"*50)\n",
    "    for idx, paragraph in enumerate(doc.paragraphs):\n",
    "        if paragraph.text.strip():  # Only print non-empty paragraphs\n",
    "            print(f\"[{idx}] {paragraph.text}\")\n",
    "            print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "859e9f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "docx_path1=\"./Faseeh Resume GA11.docx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "956200bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_all_paragraphs(docx_path: str):\n",
    "    \"\"\"\n",
    "    Print all paragraphs from the DOCX file with their index and text\n",
    "    \"\"\"\n",
    "    doc = Document(docx_path)\n",
    "    \n",
    "    print(\"\\nAll Paragraphs in Document:\")\n",
    "    print(\"=\"*50)\n",
    "    for idx, paragraph in enumerate(doc.paragraphs):\n",
    "        if paragraph.text.strip():  # Only print non-empty paragraphs\n",
    "            print(f\"[{idx}] {paragraph.text}  \")\n",
    "            if paragraph.hyperlinks:\n",
    "                    for hyperlink in paragraph.hyperlinks:\n",
    "                        print(f\"Text: {hyperlink.text}\")\n",
    "                        print(f\"URL: {hyperlink.url}\")\n",
    "            print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "eb2e41b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All Paragraphs in Document:\n",
      "==================================================\n",
      "[0]                             Mohammad Faseeh Ahmed  \n",
      "--------------------------------------------------\n",
      "[1] mm9314@g.rit.edu | +1 585 202 5217 | LinkedIn | Github | Portfolio | Kaggle | Tableau   \n",
      "Text: mm9314@g.rit.edu\n",
      "URL: mailto:mm9314@g.rit.edu\n",
      "Text: LinkedIn \n",
      "URL: https://www.linkedin.com/in/mohammad-faseeh-ahmed/\n",
      "Text: Github\n",
      "URL: https://github.com/faseehahmed26\n",
      "Text: Portfolio\n",
      "URL: https://faseehahmed26.github.io/portfolio/\n",
      "Text: Kaggle\n",
      "URL: https://www.kaggle.com/faseeh001\n",
      "Text: Tableau\n",
      "URL: https://public.tableau.com/app/profile/faseeh5112\n",
      "--------------------------------------------------\n",
      "[3] EDUCATION  \n",
      "--------------------------------------------------\n",
      "[5] Rochester Institute of Technology, Rochester, NY,  M.S in Data Science\t      Expected May 2025  \n",
      "--------------------------------------------------\n",
      "[6] Coursework: Neural Networks, Software Engineering for Data Science, Applied Statistics.                         GPA: 3.84/4.00  \n",
      "--------------------------------------------------\n",
      "[7] Jawaharlal Nehru Technological University Hyderabad, B.Tech in Computer Science\t July 2018 - July 2022  \n",
      "--------------------------------------------------\n",
      "[8] Coursework: Data Structures and Algorithms, Computer Vision, Artificial Intelligence, NLP \t         GPA: 3.2/4.00  \n",
      "--------------------------------------------------\n",
      "[9] SKILLS  \n",
      "--------------------------------------------------\n",
      "[11] Programming Languages: Java, Python, C++, R, JavaScript, Object Oriented Programming(Python, Java)  \n",
      "--------------------------------------------------\n",
      "[12] Frameworks: PyTorch, Keras, Scikit, Tensorflow, Groovy, PySpark, Flask, React, NodeJS,  \n",
      "--------------------------------------------------\n",
      "[13] Databases: SQL, MongoDB, SQLite, MySQL, NoSQL, PostgreSQL, DynamoDB  \n",
      "--------------------------------------------------\n",
      "[14] Technologies/Tools: JSON, Docker, Git, AWS, Kafka, GitLab, Numpy, Pandas, MLflow, Postman, Tableau, Power BI, MLOps  \n",
      "--------------------------------------------------\n",
      "[15] ML Algorithms and Techniques: Regression, Classification, Clustering, Recommender Systems, Deep Learning, NLP, A/B Testing, Time Series, Optimization, EDA, ETL,   \n",
      "--------------------------------------------------\n",
      "[17] PROFESSIONAL EXPERIENCE AND INTERNSHIPS  \n",
      "--------------------------------------------------\n",
      "[20] Daiichi Sankyo Inc, Basking Ridge, NJ- R&D Data Governance Intern\t        03/2024 - Present  \n",
      "--------------------------------------------------\n",
      "[21] Developed an Informed Consent Forms (ICF) analysis tool using BERT and T5 models, also testing Amazon Bedrock for large-scale language processing.  \n",
      "--------------------------------------------------\n",
      "[22] Built a Flask-based frontend for the ICF tool, enabling secure modifications of document classifications based on user permissions.  \n",
      "--------------------------------------------------\n",
      "[23] Implemented advanced Large Language Models (LLMs) using Amazon Bedrock, achieving a 20% increase in classification accuracy for detecting explicit prohibitions against data sharing.  \n",
      "--------------------------------------------------\n",
      "[24] Utilized Amazon SageMaker for model training and fine-tuning, streamlining the handling of vast volumes of legal documents to ensure scalability and efficiency.  \n",
      "--------------------------------------------------\n",
      "[25] Optimized model inference and processing times with EC2 instances, improving overall performance and reliability of the ICF analysis tool.  \n",
      "--------------------------------------------------\n",
      "[26] 1. Designed an on-premise chatbot delivering instant trial data, cutting access time by 70%.    \n",
      "--------------------------------------------------\n",
      "[27] 2. Enabled visualizations in the chatbot, improving data comprehension and decision-making efficiency.    \n",
      "--------------------------------------------------\n",
      "[28] 3. Automated milestone and site tracking, enhancing study performance and reducing missed deadlines by 30%.    \n",
      "--------------------------------------------------\n",
      "[29] 4. Developed a secure chatbot architecture, ensuring 100% compliance with company data regulations.    \n",
      "--------------------------------------------------\n",
      "[30] 5. Integrated RAG and AI, achieving 80% improvement in answer accuracy and user trust.    \n",
      "--------------------------------------------------\n",
      "[31] Led the migration of clinical data from Veeva Vault to Redshift, establishing an ETL pipeline that ensured data accuracy and improved analytics accessibility.  \n",
      "--------------------------------------------------\n",
      "[32] Built indexing and foreign key relationships in Redshift to optimize query performance, enabling high-speed analysis on clinical data.  \n",
      "--------------------------------------------------\n",
      "[33] Implemented data validation checks during migration to uphold data governance and regulatory compliance, ensuring reliable and accurate data in the new environment.  \n",
      "--------------------------------------------------\n",
      "[34] Collaborating with stakeholders to align the tool's capabilities with Daiichi Sankyo's data-sharing policies, compliance standards, and real-world data requirements.  \n",
      "--------------------------------------------------\n",
      "[35] Rochester Institute Of Technology, Rochester, NY- Research Assistant\t        08/2024 - Present  \n",
      "--------------------------------------------------\n",
      "[36] Collaborating with Professor Haibo Yang to enhance federated learning models, utilizing gRPC and PyTorch to implement scalable decentralized training algorithms.  \n",
      "--------------------------------------------------\n",
      "[37] Analyzing and optimizing distributed environments with PyTorch RPC, reducing communication overhead by 15% through improved data transfer protocols.  \n",
      "--------------------------------------------------\n",
      "[38] Leading the integration of the FedDisco algorithm into existing frameworks, accelerating model convergence and reducing training time by 20%.  \n",
      "--------------------------------------------------\n",
      "[39] Transitioning between distributed communication algorithms, leveraging FedDisco's adaptive communication strategies for efficient, real-time model updates.  \n",
      "--------------------------------------------------\n",
      "[40] Developing advanced distributed training techniques for Large Language Models (LLMs), achieving a 30% increase in system performance and minimizing network latency.  \n",
      "--------------------------------------------------\n",
      "[41] Utilizing gRPC for dynamic, secure communication within federated learning models, enhancing data synchronization and increasing overall model accuracy by 10%.  \n",
      "--------------------------------------------------\n",
      "[42] Designing hands-on projects for the Intro to Machine Learning course, applying theory to real-world machine learning scenarios using Python and PyTorch.  \n",
      "--------------------------------------------------\n",
      "[43] Providing personalized support to over 25 students, fostering problem-solving skills and guiding successful project outcomes, significantly improving their understanding of distributed machine learning concepts.  \n",
      "--------------------------------------------------\n",
      "[44] SEO Content AI, Los Angeles, CA - AI Infrastructure Engineer  \t Nov 2022 - July 2023  \n",
      "--------------------------------------------------\n",
      "[46] Enhanced AI-driven content generation efficiency by 25% through leading the integration of transformers within AWS microservices.  \n",
      "--------------------------------------------------\n",
      "[47] Developed a Chrome extension using Python, JavaScript, and NodeJS, boosting content quality and generation speed by 40%.  \n",
      "--------------------------------------------------\n",
      "[48] Utilized Docker for scalable and reliable deployment across cloud environments via AWS ECS and Fargate.  \n",
      "--------------------------------------------------\n",
      "[49] Improved project management and team collaboration through implementing effective development methodologies.  \n",
      "--------------------------------------------------\n",
      "[50] Optimized content creation for various use cases by conducting A/B testing with models like LLaMA-13B, Flan-T5, Vicuna-13B, and GPT-3.5.  \n",
      "--------------------------------------------------\n",
      "[51] Broadened audience reach by using GCP's Google Translate API for multi-language content translation.  \n",
      "--------------------------------------------------\n",
      "[52] Ensured high-quality, error-free content by implementing NLP algorithms with BERT, RoBERTa, and DistilBERT for grammar correction, significantly enhancing user trust.  \n",
      "--------------------------------------------------\n",
      "[53] White Label Resell, Los Angeles, CA - Machine Learning Engineer                                      June 2022 - March 2023  \n",
      "--------------------------------------------------\n",
      "[55] Automated article generation for digital marketing using AWS Lambda, NodeJS, and API Gateway achieving a 60x reduction in operational costs.  \n",
      "--------------------------------------------------\n",
      "[56] Integrated NLP and TensorFlow to analyze and generate over 130K high-quality articles within a week, improving content strategy.  \n",
      "--------------------------------------------------\n",
      "[57] Utilized DynamoDB for managing large datasets, ensuring efficient data access and manipulation for content generation processes.  \n",
      "--------------------------------------------------\n",
      "[58] Fine-tuned advanced NLP models like BERT, RoBERTa, ALBERT, and DistilBERT for contextual understanding, improving the relevance and quality of generated articles.  \n",
      "--------------------------------------------------\n",
      "[59] Explored generative models including GPT-3, GPT-Neo, GPT-J, and GPT-NeoX, to diversify content creation, ensuring a wide range of engaging and unique articles.  \n",
      "--------------------------------------------------\n",
      "[60] Leveraged Flan-T5 and T5 Codegen for specific content generation tasks, optimizing for both efficiency and creativity in article production.  \n",
      "--------------------------------------------------\n",
      "[61] Developed custom dataset models with Gensim, enhancing thematic accuracy and alignment with marketing goals.  \n",
      "--------------------------------------------------\n",
      "[62] Employed MLOps practices with Git and Docker to streamline the development, training, and deployment of machine learning models, facilitating continuous improvement and collaboration.  \n",
      "--------------------------------------------------\n",
      "[64] Digital Clinics Research and Services, Hyderabad, India - Data Scientist Intern\tNov 2021 - Dec 2022  \n",
      "--------------------------------------------------\n",
      "[66] Developed a sophisticated image classification system using Faster R-CNN, tailored for the nuanced detection of cancerous cells in histopathological images, leveraging TensorFlow for model training and optimization.  \n",
      "--------------------------------------------------\n",
      "[67] Engineered a custom segmentation solution with Detectron2, enabling precise delineation of tumor boundaries in medical scans. This approach utilized QuPath for image management and Groovy for scripting complex analysis workflows, significantly enhancing the quality of medical image analysis.  \n",
      "--------------------------------------------------\n",
      "[68] Implemented YOLOv5 for the automated screening of pathological slides, achieving unprecedented speed and accuracy in identifying critical diagnostic markers, integrated with OpenCV for real-time image processing and enhancements.  \n",
      "--------------------------------------------------\n",
      "[69] Led the integration of MLOps practices, utilizing Docker for containerization and AWS for model deployment, ensuring scalable and robust AI solutions in clinical settings.  \n",
      "--------------------------------------------------\n",
      "[70] Mentored a diverse group of 10-20 junior colleagues (first and second-year students) on multiple ML and DL projects, fostering a collaborative learning environment and enhancing the team's overall AI expertise and project execution capabilities.  \n",
      "--------------------------------------------------\n",
      "[72] Edgeforce Solutions, Hyderabad, India - Data Scientist Intern\tNov 2021 - Feb 2022  \n",
      "--------------------------------------------------\n",
      "[74] Built a YOLOv5-based real-time object detection system using Python and TensorFlow, integrated with Streamlit for a user-friendly interface, achieving 90% accuracy.  \n",
      "--------------------------------------------------\n",
      "[75] Engineered a deep learning model for speech recognition with PyTorch, achieving 95% accuracy, deployed in an Army Walkie Talkie emulator.  \n",
      "--------------------------------------------------\n",
      "[76] Utilized Docker and AWS for deploying scalable and efficient machine learning models, reducing operational costs and improving resource utilization.  \n",
      "--------------------------------------------------\n",
      "[77] Leveraged SQL and SQLite for efficient data storage and retrieval, enhancing system performance and data management.  \n",
      "--------------------------------------------------\n",
      "[78] Conducted A/B testing to optimize the model's performance, resulting in enhanced accuracy and user satisfaction.  \n",
      "--------------------------------------------------\n",
      "[81] PROJECTS  \n",
      "--------------------------------------------------\n",
      "[83] Chronic Kidney Disease Predictor \t Nov 2022 - July 20\t   \n",
      "--------------------------------------------------\n",
      "[85] Utilized Python's Pandas and Numpy for data manipulation and preprocessing, ensuring accurate model inputs.  \n",
      "--------------------------------------------------\n",
      "[86] Implemented Scikit-learn to develop a logistic regression model, achieving a 98% F1 score for kidney disease prediction.  \n",
      "--------------------------------------------------\n",
      "[87] Orchestrated a hybrid application structure, employing Flask for backend functionalities and leveraging the MERN stack for frontend development, to provide an intuitive platform for disease prediction.  \n",
      "--------------------------------------------------\n",
      "[88] Employed Matplotlib and Seaborn for data visualization, providing insightful analytics for model performance evaluation.  \n",
      "--------------------------------------------------\n",
      "[89] Applied MongoDB for efficient data storage, enabling scalable and secure management of patient data.  \n",
      "--------------------------------------------------\n",
      "[90] Beverage Management System                                                                                        June 2022 - March 2023  \n",
      "--------------------------------------------------\n",
      "[92] Implemented TensorFlow for creating a convolutional neural network model, achieving real-time beverage detection with 98% accuracy.  \n",
      "--------------------------------------------------\n",
      "[93] Used OpenCV for image processing, enhancing the model's ability to recognize various beverages under different conditions.  \n",
      "--------------------------------------------------\n",
      "[94] Developed a web application using Flask as the backend and ReactJS for the frontend, offering a seamless user experience.  \n",
      "--------------------------------------------------\n",
      "[95] Leveraged PostgreSQL for database management, ensuring robust and reliable storage of inventory data.  \n",
      "--------------------------------------------------\n",
      "[96] Incorporated Docker for deploying the application, ensuring consistency across development and production environments.  \n",
      "--------------------------------------------------\n",
      "[97] Covid19 Bot  \t Nov 2022 - July 20\t   \n",
      "--------------------------------------------------\n",
      "[99] Developed a chatbot using Python, Flask, and DialogFlow to offer real-time COVID-19 updates via Telegram, enhancing accessibility for users.  \n",
      "--------------------------------------------------\n",
      "[100] Implemented Pandas and Numpy for robust data analysis and numerical computations to provide accurate and up-to-date health statistics.  \n",
      "--------------------------------------------------\n",
      "[101] Integrated MongoDB to efficiently store and manage user queries and responses, facilitating personalized interaction and data retrieval.  \n",
      "--------------------------------------------------\n",
      "[102] Utilized RapidAPI for accessing live COVID-19 data, ensuring the chatbot's information was current and reliable for users seeking instant updates.  \n",
      "--------------------------------------------------\n",
      "[104] Flight Price Predictor  \t Nov 2022 - July 20\t   \n",
      "--------------------------------------------------\n",
      "[106] Enhanced the predictive model using Scikit-learn's regression techniques and LSTM networks for dynamic price forecasting, incorporating temporal data analysis.  \n",
      "--------------------------------------------------\n",
      "[107] Conducted extensive feature engineering with Pandas, improving model accuracy by identifying key price determinants from historical flight data.  \n",
      "--------------------------------------------------\n",
      "[108] Utilized Matplotlib and Seaborn for insightful visualizations of data trends and model performance, aiding in the interpretability of predictions.  \n",
      "--------------------------------------------------\n",
      "[109] Deployed the enhanced model on Heroku with a Flask web application interface, providing users with real-time, accurate flight price predictions.  \n",
      "--------------------------------------------------\n",
      "[110] Generating Synthetic Anime Data                                                                                        June 2022 - March 2023  \n",
      "--------------------------------------------------\n",
      "[112] Leveraged PyTorch and a DC GAN architecture to create high-quality synthetic anime images, significantly enhancing the diversity of our machine learning datasets.  \n",
      "--------------------------------------------------\n",
      "[113] Utilized OpenCV for advanced image preprocessing and augmentation tasks, including resizing, normalization, and color adjustments, to prepare images for training, enhancing the model's ability to generate diverse and realistic anime characters.  \n",
      "--------------------------------------------------\n",
      "[114] Developed Python scripts to automate the collection and preprocessing of anime images, streamlining the dataset creation process and enabling the efficient generation of large volumes of training data.  \n",
      "--------------------------------------------------\n",
      "[115] Applied OpenCV's edge detection and image filtering capabilities to enhance the detail and quality of generated images, ensuring a high degree of realism and variability in the synthetic data.  \n",
      "--------------------------------------------------\n",
      "[116] Integrated additional data augmentation techniques such as flipping, rotation, and scaling through OpenCV, further augmenting the training dataset to improve the robustness and generalization of the GAN model.  \n",
      "--------------------------------------------------\n",
      "[118] Number Plate Detection  \t Nov 2022 - July 20\t   \n",
      "--------------------------------------------------\n",
      "[120] Deployed YOLOv5 for real-time number plate detection, achieving high accuracy in diverse lighting and environmental conditions.  \n",
      "--------------------------------------------------\n",
      "[121] Incorporated OpenCV for advanced image processing tasks, such as adaptive thresholding and contour detection, to improve the accuracy of number plate recognition.  \n",
      "--------------------------------------------------\n",
      "[122] Developed a system using Flask as the backend, enabling real-time detection and information processing, with a user-friendly interface for monitoring and alerts.  \n",
      "--------------------------------------------------\n",
      "[123] Integrated OpenCV's Gaussian Blur to reduce image noise and improve the model's detection capabilities in varying environmental conditions.  \n",
      "--------------------------------------------------\n",
      "[124] Implemented perspective transformation techniques in OpenCV to correct angles and enhance the readability of number plates, ensuring accurate recognition regardless of the vehicle's orientation.  \n",
      "--------------------------------------------------\n",
      "[129] Research Papers  \n",
      "--------------------------------------------------\n",
      "[131] Tubules Detection on Breast Carcinoma Whole Slide Images Using Artificial Intelligence (Sep 2022):    \n",
      "--------------------------------------------------\n",
      "[133] This groundbreaking research utilized advanced AI deep learning models to accurately detect tubules in breast carcinoma images. The abstract was recognized for its innovation and selected for presentation at the prestigious International C-MIMI Conference at Johns Hopkins University, underscoring the project's contribution to medical imaging and diagnostics.  \n",
      "--------------------------------------------------\n",
      "[134] Web-Based Mitosis Detection on Breast Cancer Whole Slide Images Using FasterRCNN and YOLOv5 (Dec 2022):   \n",
      "--------------------------------------------------\n",
      "[136] This significant paper, published by the Science and Information (SAI) Organization in the International Journal of Advanced Computer Science Applications (IJACA), detailed the development and application of Faster R-CNN and YOLOv5 for the detection of mitosis in breast cancer whole slide images. The research provided insights into the capabilities of these models in enhancing the accuracy and efficiency of breast cancer grading, demonstrating a pivotal step forward in the application of machine learning in healthcare.  \n",
      "--------------------------------------------------\n",
      "[137] ACHIEVEMENTS  \n",
      "--------------------------------------------------\n",
      "[139] Kaggle Notebook and Dataset Expert  \n",
      "--------------------------------------------------\n",
      "[140] Open Source Contributor at OpenVino(Intel), Ivy  \n",
      "--------------------------------------------------\n",
      "[141] Won Best Overall Hack in CSH Hacks Hackathon 2023 : Developed a Chrome extension with a streamlined front-end, leveraging AWS Lambda for backend deployment, which integrates OpenAI's GPT-3.5 to scrutinize terms and conditions, enhancing user vigilance through AWS-powered processing and API Gateway for efficient data handling, encapsulating cloud architecture expertise in delivering a user-centric solution.  \n",
      "--------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "ddf=print_all_paragraphs(docx_path)\n",
    "print(ddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "5e1a8b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_static_content(doc: Document) -> Dict:\n",
    "    \"\"\"Extract static content (header info) with hyperlinks\"\"\"\n",
    "    static_content = {\n",
    "        'contact_info': {\n",
    "            'email': '',\n",
    "            'phone': '',\n",
    "            'links': {}\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Process first paragraph for contact info\n",
    "    if doc.paragraphs[0].hyperlinks:\n",
    "        print(doc.paragraphs[0].text)\n",
    "        for hyperlink in doc.paragraphs[0].hyperlinks:\n",
    "            if '@' in hyperlink.text:\n",
    "                static_content['contact_info']['email'] = hyperlink.text\n",
    "            elif any(x in hyperlink.text.lower() for x in ['linkedin', 'github', 'portfolio', 'kaggle', 'tableau']):\n",
    "                static_content['contact_info']['links'][hyperlink.text.lower()] = hyperlink.address\n",
    "            elif any(char.isdigit() for char in hyperlink.text):\n",
    "                static_content['contact_info']['phone'] = hyperlink.text\n",
    "    try:\n",
    "        if static_content['contact_info']['phone'] =='':\n",
    "            print(doc.paragraphs[0].text.split(\"|\"))\n",
    "            static_content['contact_info']['phone']=[s.strip() for s in doc.paragraphs[0].text.split(\"|\")  if re.search(r'\\b(?:\\+?\\d[\\d\\s-]{9,})\\b', s)][0]\n",
    "    except:\n",
    "        print(\"No Number\")\n",
    "    return static_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "e55c18c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mm9314@g.rit.edu | +1 585 202 5217 | LinkedIn | Github | Portfolio | Kaggle | Tableau \n",
      "['mm9314@g.rit.edu ', ' +1 585 202 5217 ', ' LinkedIn ', ' Github ', ' Portfolio ', ' Kaggle ', ' Tableau ']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'contact_info': {'email': 'mm9314@g.rit.edu',\n",
       "  'phone': '+1 585 202 5217',\n",
       "  'links': {'linkedin ': 'https://www.linkedin.com/in/mohammad-faseeh-ahmed/',\n",
       "   'github': 'https://github.com/faseehahmed26',\n",
       "   'portfolio': 'https://faseehahmed26.github.io/portfolio/',\n",
       "   'kaggle': 'https://www.kaggle.com/faseeh001',\n",
       "   'tableau': 'https://public.tableau.com/app/profile/faseeh5112'}}}"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " extract_static_content(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bce82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_dict = {\n",
    "    'skills': {\n",
    "        'system': \"\"\"You are a resume skill optimizer that matches candidate skills with job requirements.\"\"\",\n",
    "        'user': \"\"\"Given these skills categories and the job description, return only the most relevant skills \n",
    "                  organized in the same categories. Limit to 2-3 lines per category.\n",
    "                  \n",
    "                  Format the output as a dictionary with categories as keys and lists of skills as values.\n",
    "                  \n",
    "                  Skills: {skills}\n",
    "                  Job Description: {job_description}\"\"\"\n",
    "    },\n",
    "    'experience': {\n",
    "        'system': \"\"\"You are an expert resume writer specializing in the STAR method.\"\"\",\n",
    "        'user': \"\"\"Convert these experience points into 2-4 powerful bullet points relevant to the job description.\n",
    "                  Each bullet should:\n",
    "                  - Start with a strong action verb\n",
    "                  - Include specific technologies used\n",
    "                  - Show measurable impact (%, metrics)\n",
    "                  - Be relevant to the job requirements\n",
    "                  \n",
    "                  Experience: {experience}\n",
    "                  Job Description: {job_description}\n",
    "                  \n",
    "                  Format output as a list of bullets.\"\"\"\n",
    "    },\n",
    "    'projects': {\n",
    "        'system': \"\"\"You are a technical project curator for resumes.\"\"\",\n",
    "        'user': \"\"\"Select and optimize 2 most relevant projects for this job description.\n",
    "                  For each project, provide 2-3 technical bullet points that:\n",
    "                  - Highlight relevant technologies\n",
    "                  - Show technical complexity\n",
    "                  - Include measurable outcomes\n",
    "                  \n",
    "                  Projects: {projects}\n",
    "                  Job Description: {job_description}\n",
    "                  \n",
    "                  Format as a dictionary with project names as keys and bullet lists as values.\"\"\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "c5b8e074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['+1 585 202 5217']\n"
     ]
    }
   ],
   "source": [
    "strings=['mm9314@g.rit.edu ', ' +1 585 202 5217 ', ' LinkedIn ', ' Github ', ' Portfolio ', ' Kaggle ', ' Tableau ']\n",
    "phone_numbers = [s.strip() for s in strings if re.search(r'\\b(?:\\+?\\d[\\d\\s-]{9,})\\b', s)]\n",
    "print(phone_numbers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2abcf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "docx_path = \"./Faseeh Curriculum Vitae.docx\"\n",
    "cv_data = process_cv(docx_path)\n",
    "\n",
    "# Print extracted sections\n",
    "for section, content in cv_data.items():\n",
    "    print(f\"\\n{'='*50}\\n{section.upper()}:\\n{'='*50}\")\n",
    "    if isinstance(content, dict):  # For contact info\n",
    "        for key, value in content.items():\n",
    "            print(f\"{key}: {value}\")\n",
    "    else:\n",
    "        print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd4671b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "from docx.opc.constants import RELATIONSHIP_TYPE as RT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "97bb047d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8123e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "from docx.shared import Pt, Inches\n",
    "from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
    "from typing import Dict\n",
    "\n",
    "def create_resume_docx(cv_data: Dict, output_path: str = 'generated_resume.docx'):\n",
    "    \"\"\"\n",
    "    Create a DOCX resume from structured CV data\n",
    "    Args:\n",
    "        cv_data (Dict): Dictionary containing CV sections\n",
    "        output_path (str): Path to save the generated DOCX\n",
    "    \"\"\"\n",
    "    doc = Document()\n",
    "    \n",
    "    # Set up document margins\n",
    "    sections = doc.sections\n",
    "    for section in sections:\n",
    "        section.top_margin = Inches(0.5)\n",
    "        section.bottom_margin = Inches(0.5)\n",
    "        section.left_margin = Inches(0.5)\n",
    "        section.right_margin = Inches(0.5)\n",
    "\n",
    "    # Add contact information\n",
    "    contact_info = cv_data.get('contact_info', {})\n",
    "    if 'name' in contact_info:\n",
    "        name_paragraph = doc.add_paragraph()\n",
    "        name_paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "        name_run = name_paragraph.add_run(contact_info['name'])\n",
    "        name_run.bold = True\n",
    "        name_run.font.size = Pt(12)\n",
    "\n",
    "    # Function to add a section\n",
    "    def add_section(title: str, content: str):\n",
    "        # Add section header\n",
    "        header = doc.add_paragraph()\n",
    "        header_run = header.add_run(title)\n",
    "        header_run.bold = True\n",
    "        header_run.font.size = Pt(12)\n",
    "        \n",
    "        # Add section content\n",
    "        if content:\n",
    "            content_paragraph = doc.add_paragraph()\n",
    "            content_run = content_paragraph.add_run(content)\n",
    "            content_run.font.size = Pt(10)\n",
    "\n",
    "    # Add each section\n",
    "    sections_order = [\n",
    "        ('EDUCATION', cv_data.get('education', '')),\n",
    "        ('SKILLS', cv_data.get('skills', '')),\n",
    "        ('PROFESSIONAL EXPERIENCE AND INTERNSHIPS', cv_data.get('experience', '')),\n",
    "        ('PROJECTS', cv_data.get('projects', '')),\n",
    "        ('RESEARCH PAPERS', cv_data.get('research_papers', ''))\n",
    "    ]\n",
    "\n",
    "    for title, content in sections_order:\n",
    "        add_section(title, content)\n",
    "\n",
    "    # Save the document\n",
    "    doc.save(output_path)\n",
    "    return output_path\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148a39f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "\n",
    "def extract_paragraphs_with_hyperlinks(docx_path):\n",
    "    doc = Document(docx_path)\n",
    "    paragraphs = []\n",
    "\n",
    "    for para in doc.paragraphs:\n",
    "        # Extract the full paragraph text\n",
    "        full_text = para.text\n",
    "        hyperlinks = []\n",
    "\n",
    "        # Iterate through the runs to find hyperlinks\n",
    "        for run in para.runs:\n",
    "            if run.hyperlink:\n",
    "                hyperlink = run.hyperlink\n",
    "                display_text = run.text\n",
    "                url = hyperlink.target_ref\n",
    "                hyperlinks.append((display_text, url))\n",
    "\n",
    "        paragraphs.append((full_text, hyperlinks))\n",
    "\n",
    "    return paragraphs\n",
    "\n",
    "def format_paragraphs(paragraphs):\n",
    "    formatted_output = []\n",
    "    for idx, (text, hyperlinks) in enumerate(paragraphs):\n",
    "        formatted_output.append(f\"[{idx}] {text}\")\n",
    "        for display_text, url in hyperlinks:\n",
    "            formatted_output.append(f\"Text: {display_text}\")\n",
    "            formatted_output.append(f\"URL: {url}\")\n",
    "        formatted_output.append(\"--------------------------------------------------\")\n",
    "    return \"\\n\".join(formatted_output)\n",
    "\n",
    "# Usage\n",
    "paragraphs = extract_paragraphs_with_hyperlinks(docx_path)\n",
    "formatted_text = format_paragraphs(paragraphs)\n",
    "print(formatted_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a4a273",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install docxpy \n",
    "import docxpy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803744d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = docxpy.DOCReader(docx_path)\n",
    "doc.process()  # Process the document to extract data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1e1d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.data[ 'document']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c65d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract hyperlinks\n",
    "hyperlinks = doc.data['links']\n",
    "\n",
    "# Extract paragraphs\n",
    "paragraphs = doc.data[ 'document']\n",
    "\n",
    "# Function to format and print the document content\n",
    "def print_document_content(paragraphs, hyperlinks):\n",
    "    print(\"All Paragraphs in Document:\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    for i, para in enumerate(paragraphs):\n",
    "        # Print paragraph index and content\n",
    "        print(f\"[{i}] {para}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        # Check if the paragraph contains any hyperlinks\n",
    "        if i in hyperlinks:\n",
    "            for link_text, url in hyperlinks[i]:\n",
    "                print(f\"Text: {link_text}\")\n",
    "                print(f\"URL: {url}\")\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "# Call the function to print the content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9124214f",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = Document(docx_path)\n",
    "extracted_text = []\n",
    "# print(doc.sections)\n",
    "# Extract text from the header (if present)\n",
    "for section in doc.sections:\n",
    "    header = section.header\n",
    "#         print(header)\n",
    "    for paragraph in header.paragraphs:\n",
    "\n",
    "        if paragraph.text.strip():\n",
    "            extracted_text.append(paragraph.text)\n",
    "\n",
    "# Extract text from the body\n",
    "for paragraph in doc.paragraphs:\n",
    "    if paragraph.text.strip():\n",
    "        extracted_text.append(paragraph.text)\n",
    "\n",
    "# Combine and return the text\n",
    "full_text = \"\\n\".join(extracted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e1adb7c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Document' object has no attribute 'title'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m--------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_47844\\1613737431.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'Document' object has no attribute 'title'"
     ]
    }
   ],
   "source": [
    "doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dab4eec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mm9314@g.rit.edu | +1 585 202 5217 | LinkedIn | Github | Portfolio | Kaggle | Tableau \n",
      "EDUCATION\n",
      "Rochester Institute of Technology, Rochester, NY,  M.S in Data Science\t      Expected May 2025\n",
      "Coursework: Neural Networks, Software Engineering for Data Science, Applied Statistics.                         GPA: 3.84/4.00\n",
      "Jawaharlal Nehru Technological University Hyderabad, B.Tech in Computer Science\t July 2018 - July 2022\n",
      "Coursework: Data Structures and Algorithms, Computer Vision, Artificial Intelligence, NLP \t         GPA: 3.2/4.00\n",
      "SKILLS\n",
      "Programming Languages: Java, Python, C++, R, JavaScript, Object Oriented Programming(Python, Java)\n",
      "Frameworks: PyTorch, Keras, Scikit, Tensorflow, Groovy, PySpark, Flask, React, NodeJS,\n",
      "Databases: SQL, MongoDB, SQLite, MySQL, NoSQL, PostgreSQL, DynamoDB\n",
      "Technologies/Tools: JSON, Docker, Git, AWS, Kafka, GitLab, Numpy, Pandas, MLflow, Postman, Tableau, Power BI, MLOps\n",
      "ML Algorithms and Techniques: Regression, Classification, Clustering, Recommender Systems, Deep Learning, NLP, A/B Testing, Time Series, Optimization, EDA, ETL, \n",
      "PROFESSIONAL EXPERIENCE AND INTERNSHIPS\n",
      "Daiichi Sankyo Inc, Basking Ridge, NJ- R&D Data Governance Intern\t        03/2024 - Present\n",
      "Developed an Informed Consent Forms (ICF) analysis tool using BERT and T5 models, also testing Amazon Bedrock for large-scale language processing.\n",
      "Built a Flask-based frontend for the ICF tool, enabling secure modifications of document classifications based on user permissions.\n",
      "Implemented advanced Large Language Models (LLMs) using Amazon Bedrock, achieving a 20% increase in classification accuracy for detecting explicit prohibitions against data sharing.\n",
      "Utilized Amazon SageMaker for model training and fine-tuning, streamlining the handling of vast volumes of legal documents to ensure scalability and efficiency.\n",
      "Optimized model inference and processing times with EC2 instances, improving overall performance and reliability of the ICF analysis tool.\n",
      "1. Designed an on-premise chatbot delivering instant trial data, cutting access time by 70%.  \n",
      "2. Enabled visualizations in the chatbot, improving data comprehension and decision-making efficiency.  \n",
      "3. Automated milestone and site tracking, enhancing study performance and reducing missed deadlines by 30%.  \n",
      "4. Developed a secure chatbot architecture, ensuring 100% compliance with company data regulations.  \n",
      "5. Integrated RAG and AI, achieving 80% improvement in answer accuracy and user trust.  \n",
      "Led the migration of clinical data from Veeva Vault to Redshift, establishing an ETL pipeline that ensured data accuracy and improved analytics accessibility.\n",
      "Built indexing and foreign key relationships in Redshift to optimize query performance, enabling high-speed analysis on clinical data.\n",
      "Implemented data validation checks during migration to uphold data governance and regulatory compliance, ensuring reliable and accurate data in the new environment.\n",
      "Collaborating with stakeholders to align the tool's capabilities with Daiichi Sankyo's data-sharing policies, compliance standards, and real-world data requirements.\n",
      "Rochester Institute Of Technology, Rochester, NY- Research Assistant\t        08/2024 - Present\n",
      "Collaborating with Professor Haibo Yang to enhance federated learning models, utilizing gRPC and PyTorch to implement scalable decentralized training algorithms.\n",
      "Analyzing and optimizing distributed environments with PyTorch RPC, reducing communication overhead by 15% through improved data transfer protocols.\n",
      "Leading the integration of the FedDisco algorithm into existing frameworks, accelerating model convergence and reducing training time by 20%.\n",
      "Transitioning between distributed communication algorithms, leveraging FedDisco's adaptive communication strategies for efficient, real-time model updates.\n",
      "Developing advanced distributed training techniques for Large Language Models (LLMs), achieving a 30% increase in system performance and minimizing network latency.\n",
      "Utilizing gRPC for dynamic, secure communication within federated learning models, enhancing data synchronization and increasing overall model accuracy by 10%.\n",
      "Designing hands-on projects for the Intro to Machine Learning course, applying theory to real-world machine learning scenarios using Python and PyTorch.\n",
      "Providing personalized support to over 25 students, fostering problem-solving skills and guiding successful project outcomes, significantly improving their understanding of distributed machine learning concepts.\n",
      "SEO Content AI, Los Angeles, CA - AI Infrastructure Engineer  \t Nov 2022 - July 2023\n",
      "Enhanced AI-driven content generation efficiency by 25% through leading the integration of transformers within AWS microservices.\n",
      "Developed a Chrome extension using Python, JavaScript, and NodeJS, boosting content quality and generation speed by 40%.\n",
      "Utilized Docker for scalable and reliable deployment across cloud environments via AWS ECS and Fargate.\n",
      "Improved project management and team collaboration through implementing effective development methodologies.\n",
      "Optimized content creation for various use cases by conducting A/B testing with models like LLaMA-13B, Flan-T5, Vicuna-13B, and GPT-3.5.\n",
      "Broadened audience reach by using GCP's Google Translate API for multi-language content translation.\n",
      "Ensured high-quality, error-free content by implementing NLP algorithms with BERT, RoBERTa, and DistilBERT for grammar correction, significantly enhancing user trust.\n",
      "White Label Resell, Los Angeles, CA - Machine Learning Engineer                                      June 2022 - March 2023\n",
      "Automated article generation for digital marketing using AWS Lambda, NodeJS, and API Gateway achieving a 60x reduction in operational costs.\n",
      "Integrated NLP and TensorFlow to analyze and generate over 130K high-quality articles within a week, improving content strategy.\n",
      "Utilized DynamoDB for managing large datasets, ensuring efficient data access and manipulation for content generation processes.\n",
      "Fine-tuned advanced NLP models like BERT, RoBERTa, ALBERT, and DistilBERT for contextual understanding, improving the relevance and quality of generated articles.\n",
      "Explored generative models including GPT-3, GPT-Neo, GPT-J, and GPT-NeoX, to diversify content creation, ensuring a wide range of engaging and unique articles.\n",
      "Leveraged Flan-T5 and T5 Codegen for specific content generation tasks, optimizing for both efficiency and creativity in article production.\n",
      "Developed custom dataset models with Gensim, enhancing thematic accuracy and alignment with marketing goals.\n",
      "Employed MLOps practices with Git and Docker to streamline the development, training, and deployment of machine learning models, facilitating continuous improvement and collaboration.\n",
      "Digital Clinics Research and Services, Hyderabad, India - Data Scientist Intern\tNov 2021 - Dec 2022\n",
      "Developed a sophisticated image classification system using Faster R-CNN, tailored for the nuanced detection of cancerous cells in histopathological images, leveraging TensorFlow for model training and optimization.\n",
      "Engineered a custom segmentation solution with Detectron2, enabling precise delineation of tumor boundaries in medical scans. This approach utilized QuPath for image management and Groovy for scripting complex analysis workflows, significantly enhancing the quality of medical image analysis.\n",
      "Implemented YOLOv5 for the automated screening of pathological slides, achieving unprecedented speed and accuracy in identifying critical diagnostic markers, integrated with OpenCV for real-time image processing and enhancements.\n",
      "Led the integration of MLOps practices, utilizing Docker for containerization and AWS for model deployment, ensuring scalable and robust AI solutions in clinical settings.\n",
      "Mentored a diverse group of 10-20 junior colleagues (first and second-year students) on multiple ML and DL projects, fostering a collaborative learning environment and enhancing the team's overall AI expertise and project execution capabilities.\n",
      "Edgeforce Solutions, Hyderabad, India - Data Scientist Intern\tNov 2021 - Feb 2022\n",
      "Built a YOLOv5-based real-time object detection system using Python and TensorFlow, integrated with Streamlit for a user-friendly interface, achieving 90% accuracy.\n",
      "Engineered a deep learning model for speech recognition with PyTorch, achieving 95% accuracy, deployed in an Army Walkie Talkie emulator.\n",
      "Utilized Docker and AWS for deploying scalable and efficient machine learning models, reducing operational costs and improving resource utilization.\n",
      "Leveraged SQL and SQLite for efficient data storage and retrieval, enhancing system performance and data management.\n",
      "Conducted A/B testing to optimize the model's performance, resulting in enhanced accuracy and user satisfaction.\n",
      "PROJECTS\n",
      "Chronic Kidney Disease Predictor \t Nov 2022 - July 20\t \n",
      "Utilized Python's Pandas and Numpy for data manipulation and preprocessing, ensuring accurate model inputs.\n",
      "Implemented Scikit-learn to develop a logistic regression model, achieving a 98% F1 score for kidney disease prediction.\n",
      "Orchestrated a hybrid application structure, employing Flask for backend functionalities and leveraging the MERN stack for frontend development, to provide an intuitive platform for disease prediction.\n",
      "Employed Matplotlib and Seaborn for data visualization, providing insightful analytics for model performance evaluation.\n",
      "Applied MongoDB for efficient data storage, enabling scalable and secure management of patient data.\n",
      "Beverage Management System                                                                                        June 2022 - March 2023\n",
      "Implemented TensorFlow for creating a convolutional neural network model, achieving real-time beverage detection with 98% accuracy.\n",
      "Used OpenCV for image processing, enhancing the model's ability to recognize various beverages under different conditions.\n",
      "Developed a web application using Flask as the backend and ReactJS for the frontend, offering a seamless user experience.\n",
      "Leveraged PostgreSQL for database management, ensuring robust and reliable storage of inventory data.\n",
      "Incorporated Docker for deploying the application, ensuring consistency across development and production environments.\n",
      "Covid19 Bot  \t Nov 2022 - July 20\t \n",
      "Developed a chatbot using Python, Flask, and DialogFlow to offer real-time COVID-19 updates via Telegram, enhancing accessibility for users.\n",
      "Implemented Pandas and Numpy for robust data analysis and numerical computations to provide accurate and up-to-date health statistics.\n",
      "Integrated MongoDB to efficiently store and manage user queries and responses, facilitating personalized interaction and data retrieval.\n",
      "Utilized RapidAPI for accessing live COVID-19 data, ensuring the chatbot's information was current and reliable for users seeking instant updates.\n",
      "Flight Price Predictor  \t Nov 2022 - July 20\t \n",
      "Enhanced the predictive model using Scikit-learn's regression techniques and LSTM networks for dynamic price forecasting, incorporating temporal data analysis.\n",
      "Conducted extensive feature engineering with Pandas, improving model accuracy by identifying key price determinants from historical flight data.\n",
      "Utilized Matplotlib and Seaborn for insightful visualizations of data trends and model performance, aiding in the interpretability of predictions.\n",
      "Deployed the enhanced model on Heroku with a Flask web application interface, providing users with real-time, accurate flight price predictions.\n",
      "Generating Synthetic Anime Data                                                                                        June 2022 - March 2023\n",
      "Leveraged PyTorch and a DC GAN architecture to create high-quality synthetic anime images, significantly enhancing the diversity of our machine learning datasets.\n",
      "Utilized OpenCV for advanced image preprocessing and augmentation tasks, including resizing, normalization, and color adjustments, to prepare images for training, enhancing the model's ability to generate diverse and realistic anime characters.\n",
      "Developed Python scripts to automate the collection and preprocessing of anime images, streamlining the dataset creation process and enabling the efficient generation of large volumes of training data.\n",
      "Applied OpenCV's edge detection and image filtering capabilities to enhance the detail and quality of generated images, ensuring a high degree of realism and variability in the synthetic data.\n",
      "Integrated additional data augmentation techniques such as flipping, rotation, and scaling through OpenCV, further augmenting the training dataset to improve the robustness and generalization of the GAN model.\n",
      "Number Plate Detection  \t Nov 2022 - July 20\t \n",
      "Deployed YOLOv5 for real-time number plate detection, achieving high accuracy in diverse lighting and environmental conditions.\n",
      "Incorporated OpenCV for advanced image processing tasks, such as adaptive thresholding and contour detection, to improve the accuracy of number plate recognition.\n",
      "Developed a system using Flask as the backend, enabling real-time detection and information processing, with a user-friendly interface for monitoring and alerts.\n",
      "Integrated OpenCV's Gaussian Blur to reduce image noise and improve the model's detection capabilities in varying environmental conditions.\n",
      "Implemented perspective transformation techniques in OpenCV to correct angles and enhance the readability of number plates, ensuring accurate recognition regardless of the vehicle's orientation.\n",
      "Research Papers\n",
      "Tubules Detection on Breast Carcinoma Whole Slide Images Using Artificial Intelligence (Sep 2022):  \n",
      "This groundbreaking research utilized advanced AI deep learning models to accurately detect tubules in breast carcinoma images. The abstract was recognized for its innovation and selected for presentation at the prestigious International C-MIMI Conference at Johns Hopkins University, underscoring the project's contribution to medical imaging and diagnostics.\n",
      "Web-Based Mitosis Detection on Breast Cancer Whole Slide Images Using FasterRCNN and YOLOv5 (Dec 2022): \n",
      "This significant paper, published by the Science and Information (SAI) Organization in the International Journal of Advanced Computer Science Applications (IJACA), detailed the development and application of Faster R-CNN and YOLOv5 for the detection of mitosis in breast cancer whole slide images. The research provided insights into the capabilities of these models in enhancing the accuracy and efficiency of breast cancer grading, demonstrating a pivotal step forward in the application of machine learning in healthcare.\n",
      "ACHIEVEMENTS\n",
      "Kaggle Notebook and Dataset Expert\n",
      "Open Source Contributor at OpenVino(Intel), Ivy\n",
      "Won Best Overall Hack in CSH Hacks Hackathon 2023 : Developed a Chrome extension with a streamlined front-end, leveraging AWS Lambda for backend deployment, which integrates OpenAI's GPT-3.5 to scrutinize terms and conditions, enhancing user vigilance through AWS-powered processing and API Gateway for efficient data handling, encapsulating cloud architecture expertise in delivering a user-centric solution.\n"
     ]
    }
   ],
   "source": [
    "# Extract text from the body\n",
    "for paragraph in doc.paragraphs:\n",
    "    if paragraph.text.strip():\n",
    "        print(paragraph.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "425fae0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SKILLS\n",
      "PROFESSIONAL EXPERIENCE AND INTERNSHIPS\n",
      "\n",
      "PROJECTS\n",
      "Research Papers\n",
      "ACHIEVEMENTS\n"
     ]
    }
   ],
   "source": [
    "def iter_headings(paragraphs):\n",
    "    for paragraph in paragraphs:\n",
    "        if paragraph.style.name.startswith('Heading'):\n",
    "            yield paragraph\n",
    "\n",
    "for heading in iter_headings(doc.paragraphs):\n",
    "    print(heading.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6841b4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mm9314@g.rit.edu | +1 585 202 5217 | LinkedIn | Github | Portfolio | Kaggle | Tableau \n",
      "EDUCATION\n",
      "Rochester Institute of Technology, Rochester, NY,  M.S in Data Science\t      Expected May 2025\n",
      "Coursework: Neural Networks, Software Engineering for Data Science, Applied Statistics.                         GPA: 3.84/4.00\n",
      "Jawaharlal Nehru Technological University Hyderabad, B.Tech in Computer Science\t July 2018 - July 2022\n",
      "Coursework: Data Structures and Algorithms, Computer Vision, Artificial Intelligence, NLP \t         GPA: 3.2/4.00\n",
      "SKILLS\n",
      "Programming Languages: Java, Python, C++, R, JavaScript, Object Oriented Programming(Python, Java)\n",
      "Frameworks: PyTorch, Keras, Scikit, Tensorflow, Groovy, PySpark, Flask, React, NodeJS,\n",
      "Databases: SQL, MongoDB, SQLite, MySQL, NoSQL, PostgreSQL, DynamoDB\n",
      "Technologies/Tools: JSON, Docker, Git, AWS, Kafka, GitLab, Numpy, Pandas, MLflow, Postman, Tableau, Power BI, MLOps\n",
      "ML Algorithms and Techniques: Regression, Classification, Clustering, Recommender Systems, Deep Learning, NLP, A/B Testing, Time Series, Optimization, EDA, ETL, \n",
      "PROFESSIONAL EXPERIENCE AND INTERNSHIPS\n",
      "Daiichi Sankyo Inc, Basking Ridge, NJ- R&D Data Governance Intern\t        03/2024 - Present\n",
      "Developed an Informed Consent Forms (ICF) analysis tool using BERT and T5 models, also testing Amazon Bedrock for large-scale language processing.\n",
      "Built a Flask-based frontend for the ICF tool, enabling secure modifications of document classifications based on user permissions.\n",
      "Implemented advanced Large Language Models (LLMs) using Amazon Bedrock, achieving a 20% increase in classification accuracy for detecting explicit prohibitions against data sharing.\n",
      "Utilized Amazon SageMaker for model training and fine-tuning, streamlining the handling of vast volumes of legal documents to ensure scalability and efficiency.\n",
      "Optimized model inference and processing times with EC2 instances, improving overall performance and reliability of the ICF analysis tool.\n",
      "1. Designed an on-premise chatbot delivering instant trial data, cutting access time by 70%.  \n",
      "2. Enabled visualizations in the chatbot, improving data comprehension and decision-making efficiency.  \n",
      "3. Automated milestone and site tracking, enhancing study performance and reducing missed deadlines by 30%.  \n",
      "4. Developed a secure chatbot architecture, ensuring 100% compliance with company data regulations.  \n",
      "5. Integrated RAG and AI, achieving 80% improvement in answer accuracy and user trust.  \n",
      "Led the migration of clinical data from Veeva Vault to Redshift, establishing an ETL pipeline that ensured data accuracy and improved analytics accessibility.\n",
      "Built indexing and foreign key relationships in Redshift to optimize query performance, enabling high-speed analysis on clinical data.\n",
      "Implemented data validation checks during migration to uphold data governance and regulatory compliance, ensuring reliable and accurate data in the new environment.\n",
      "Collaborating with stakeholders to align the tool's capabilities with Daiichi Sankyo's data-sharing policies, compliance standards, and real-world data requirements.\n",
      "Rochester Institute Of Technology, Rochester, NY- Research Assistant\t        08/2024 - Present\n",
      "Collaborating with Professor Haibo Yang to enhance federated learning models, utilizing gRPC and PyTorch to implement scalable decentralized training algorithms.\n",
      "Analyzing and optimizing distributed environments with PyTorch RPC, reducing communication overhead by 15% through improved data transfer protocols.\n",
      "Leading the integration of the FedDisco algorithm into existing frameworks, accelerating model convergence and reducing training time by 20%.\n",
      "Transitioning between distributed communication algorithms, leveraging FedDisco's adaptive communication strategies for efficient, real-time model updates.\n",
      "Developing advanced distributed training techniques for Large Language Models (LLMs), achieving a 30% increase in system performance and minimizing network latency.\n",
      "Utilizing gRPC for dynamic, secure communication within federated learning models, enhancing data synchronization and increasing overall model accuracy by 10%.\n",
      "Designing hands-on projects for the Intro to Machine Learning course, applying theory to real-world machine learning scenarios using Python and PyTorch.\n",
      "Providing personalized support to over 25 students, fostering problem-solving skills and guiding successful project outcomes, significantly improving their understanding of distributed machine learning concepts.\n",
      "SEO Content AI, Los Angeles, CA - AI Infrastructure Engineer  \t Nov 2022 - July 2023\n",
      "Enhanced AI-driven content generation efficiency by 25% through leading the integration of transformers within AWS microservices.\n",
      "Developed a Chrome extension using Python, JavaScript, and NodeJS, boosting content quality and generation speed by 40%.\n",
      "Utilized Docker for scalable and reliable deployment across cloud environments via AWS ECS and Fargate.\n",
      "Improved project management and team collaboration through implementing effective development methodologies.\n",
      "Optimized content creation for various use cases by conducting A/B testing with models like LLaMA-13B, Flan-T5, Vicuna-13B, and GPT-3.5.\n",
      "Broadened audience reach by using GCP's Google Translate API for multi-language content translation.\n",
      "Ensured high-quality, error-free content by implementing NLP algorithms with BERT, RoBERTa, and DistilBERT for grammar correction, significantly enhancing user trust.\n",
      "White Label Resell, Los Angeles, CA - Machine Learning Engineer                                      June 2022 - March 2023\n",
      "Automated article generation for digital marketing using AWS Lambda, NodeJS, and API Gateway achieving a 60x reduction in operational costs.\n",
      "Integrated NLP and TensorFlow to analyze and generate over 130K high-quality articles within a week, improving content strategy.\n",
      "Utilized DynamoDB for managing large datasets, ensuring efficient data access and manipulation for content generation processes.\n",
      "Fine-tuned advanced NLP models like BERT, RoBERTa, ALBERT, and DistilBERT for contextual understanding, improving the relevance and quality of generated articles.\n",
      "Explored generative models including GPT-3, GPT-Neo, GPT-J, and GPT-NeoX, to diversify content creation, ensuring a wide range of engaging and unique articles.\n",
      "Leveraged Flan-T5 and T5 Codegen for specific content generation tasks, optimizing for both efficiency and creativity in article production.\n",
      "Developed custom dataset models with Gensim, enhancing thematic accuracy and alignment with marketing goals.\n",
      "Employed MLOps practices with Git and Docker to streamline the development, training, and deployment of machine learning models, facilitating continuous improvement and collaboration.\n",
      "Digital Clinics Research and Services, Hyderabad, India - Data Scientist Intern\tNov 2021 - Dec 2022\n",
      "Developed a sophisticated image classification system using Faster R-CNN, tailored for the nuanced detection of cancerous cells in histopathological images, leveraging TensorFlow for model training and optimization.\n",
      "Engineered a custom segmentation solution with Detectron2, enabling precise delineation of tumor boundaries in medical scans. This approach utilized QuPath for image management and Groovy for scripting complex analysis workflows, significantly enhancing the quality of medical image analysis.\n",
      "Implemented YOLOv5 for the automated screening of pathological slides, achieving unprecedented speed and accuracy in identifying critical diagnostic markers, integrated with OpenCV for real-time image processing and enhancements.\n",
      "Led the integration of MLOps practices, utilizing Docker for containerization and AWS for model deployment, ensuring scalable and robust AI solutions in clinical settings.\n",
      "Mentored a diverse group of 10-20 junior colleagues (first and second-year students) on multiple ML and DL projects, fostering a collaborative learning environment and enhancing the team's overall AI expertise and project execution capabilities.\n",
      "Edgeforce Solutions, Hyderabad, India - Data Scientist Intern\tNov 2021 - Feb 2022\n",
      "Built a YOLOv5-based real-time object detection system using Python and TensorFlow, integrated with Streamlit for a user-friendly interface, achieving 90% accuracy.\n",
      "Engineered a deep learning model for speech recognition with PyTorch, achieving 95% accuracy, deployed in an Army Walkie Talkie emulator.\n",
      "Utilized Docker and AWS for deploying scalable and efficient machine learning models, reducing operational costs and improving resource utilization.\n",
      "Leveraged SQL and SQLite for efficient data storage and retrieval, enhancing system performance and data management.\n",
      "Conducted A/B testing to optimize the model's performance, resulting in enhanced accuracy and user satisfaction.\n",
      "PROJECTS\n",
      "Chronic Kidney Disease Predictor \t Nov 2022 - July 20\t \n",
      "Utilized Python's Pandas and Numpy for data manipulation and preprocessing, ensuring accurate model inputs.\n",
      "Implemented Scikit-learn to develop a logistic regression model, achieving a 98% F1 score for kidney disease prediction.\n",
      "Orchestrated a hybrid application structure, employing Flask for backend functionalities and leveraging the MERN stack for frontend development, to provide an intuitive platform for disease prediction.\n",
      "Employed Matplotlib and Seaborn for data visualization, providing insightful analytics for model performance evaluation.\n",
      "Applied MongoDB for efficient data storage, enabling scalable and secure management of patient data.\n",
      "Beverage Management System                                                                                        June 2022 - March 2023\n",
      "Implemented TensorFlow for creating a convolutional neural network model, achieving real-time beverage detection with 98% accuracy.\n",
      "Used OpenCV for image processing, enhancing the model's ability to recognize various beverages under different conditions.\n",
      "Developed a web application using Flask as the backend and ReactJS for the frontend, offering a seamless user experience.\n",
      "Leveraged PostgreSQL for database management, ensuring robust and reliable storage of inventory data.\n",
      "Incorporated Docker for deploying the application, ensuring consistency across development and production environments.\n",
      "Covid19 Bot  \t Nov 2022 - July 20\t \n",
      "Developed a chatbot using Python, Flask, and DialogFlow to offer real-time COVID-19 updates via Telegram, enhancing accessibility for users.\n",
      "Implemented Pandas and Numpy for robust data analysis and numerical computations to provide accurate and up-to-date health statistics.\n",
      "Integrated MongoDB to efficiently store and manage user queries and responses, facilitating personalized interaction and data retrieval.\n",
      "Utilized RapidAPI for accessing live COVID-19 data, ensuring the chatbot's information was current and reliable for users seeking instant updates.\n",
      "Flight Price Predictor  \t Nov 2022 - July 20\t \n",
      "Enhanced the predictive model using Scikit-learn's regression techniques and LSTM networks for dynamic price forecasting, incorporating temporal data analysis.\n",
      "Conducted extensive feature engineering with Pandas, improving model accuracy by identifying key price determinants from historical flight data.\n",
      "Utilized Matplotlib and Seaborn for insightful visualizations of data trends and model performance, aiding in the interpretability of predictions.\n",
      "Deployed the enhanced model on Heroku with a Flask web application interface, providing users with real-time, accurate flight price predictions.\n",
      "Generating Synthetic Anime Data                                                                                        June 2022 - March 2023\n",
      "Leveraged PyTorch and a DC GAN architecture to create high-quality synthetic anime images, significantly enhancing the diversity of our machine learning datasets.\n",
      "Utilized OpenCV for advanced image preprocessing and augmentation tasks, including resizing, normalization, and color adjustments, to prepare images for training, enhancing the model's ability to generate diverse and realistic anime characters.\n",
      "Developed Python scripts to automate the collection and preprocessing of anime images, streamlining the dataset creation process and enabling the efficient generation of large volumes of training data.\n",
      "Applied OpenCV's edge detection and image filtering capabilities to enhance the detail and quality of generated images, ensuring a high degree of realism and variability in the synthetic data.\n",
      "Integrated additional data augmentation techniques such as flipping, rotation, and scaling through OpenCV, further augmenting the training dataset to improve the robustness and generalization of the GAN model.\n",
      "Number Plate Detection  \t Nov 2022 - July 20\t \n",
      "Deployed YOLOv5 for real-time number plate detection, achieving high accuracy in diverse lighting and environmental conditions.\n",
      "Incorporated OpenCV for advanced image processing tasks, such as adaptive thresholding and contour detection, to improve the accuracy of number plate recognition.\n",
      "Developed a system using Flask as the backend, enabling real-time detection and information processing, with a user-friendly interface for monitoring and alerts.\n",
      "Integrated OpenCV's Gaussian Blur to reduce image noise and improve the model's detection capabilities in varying environmental conditions.\n",
      "Implemented perspective transformation techniques in OpenCV to correct angles and enhance the readability of number plates, ensuring accurate recognition regardless of the vehicle's orientation.\n",
      "Research Papers\n",
      "Tubules Detection on Breast Carcinoma Whole Slide Images Using Artificial Intelligence (Sep 2022):  \n",
      "This groundbreaking research utilized advanced AI deep learning models to accurately detect tubules in breast carcinoma images. The abstract was recognized for its innovation and selected for presentation at the prestigious International C-MIMI Conference at Johns Hopkins University, underscoring the project's contribution to medical imaging and diagnostics.\n",
      "Web-Based Mitosis Detection on Breast Cancer Whole Slide Images Using FasterRCNN and YOLOv5 (Dec 2022): \n",
      "This significant paper, published by the Science and Information (SAI) Organization in the International Journal of Advanced Computer Science Applications (IJACA), detailed the development and application of Faster R-CNN and YOLOv5 for the detection of mitosis in breast cancer whole slide images. The research provided insights into the capabilities of these models in enhancing the accuracy and efficiency of breast cancer grading, demonstrating a pivotal step forward in the application of machine learning in healthcare.\n",
      "ACHIEVEMENTS\n",
      "Kaggle Notebook and Dataset Expert\n",
      "Open Source Contributor at OpenVino(Intel), Ivy\n",
      "Won Best Overall Hack in CSH Hacks Hackathon 2023 : Developed a Chrome extension with a streamlined front-end, leveraging AWS Lambda for backend deployment, which integrates OpenAI's GPT-3.5 to scrutinize terms and conditions, enhancing user vigilance through AWS-powered processing and API Gateway for efficient data handling, encapsulating cloud architecture expertise in delivering a user-centric solution.\n",
      "Extracted Name: mm9314@g.rit.edu | +1 585 202 5217 | LinkedIn | Github | Portfolio | Kaggle | Tableau\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def extract_name_from_docx(docx_path: str):\n",
    "    \"\"\"\n",
    "    Extract text, including headers, to find the name or specific information.\n",
    "    \"\"\"\n",
    "    doc = Document(docx_path)\n",
    "    extracted_text = []\n",
    "    print(doc.sections\n",
    "    # Extract text from the header (if present)\n",
    "    for section in doc.sections:\n",
    "        header = section.header\n",
    "#         print(header)\n",
    "        for paragraph in header.paragraphs:\n",
    "            \n",
    "            if paragraph.text.strip():\n",
    "                extracted_text.append(paragraph.text)\n",
    "\n",
    "    # Extract text from the body\n",
    "    for paragraph in doc.paragraphs:\n",
    "        if paragraph.text.strip():\n",
    "            extracted_text.append(paragraph.text)\n",
    "\n",
    "    # Combine and return the text\n",
    "    full_text = \"\\n\".join(extracted_text)\n",
    "    return full_text\n",
    "\n",
    "# Example usage\n",
    "extracted_content = extract_name_from_docx(docx_path)\n",
    "print(extracted_content)\n",
    "\n",
    "# Extract the first non-empty line as the name (assumption-based)\n",
    "lines = extracted_content.split(\"\\n\")\n",
    "name = next((line.strip() for line in lines if line.strip()), \"Name not found\")\n",
    "print(f\"Extracted Name: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e616712",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_docx(docx_path: str):\n",
    "    \"\"\"\n",
    "    Extract text from the main body, headers, footers, and text boxes of a DOCX file.\n",
    "    \"\"\"\n",
    "    doc = Document(docx_path)\n",
    "    full_text = []\n",
    "\n",
    "    # Extract text from headers\n",
    "    for section in doc.sections:\n",
    "        header = section.header\n",
    "        for paragraph in header.paragraphs:\n",
    "            if paragraph.text.strip():\n",
    "                full_text.append(paragraph.text)\n",
    "\n",
    "    # Extract text from the main body\n",
    "    for paragraph in doc.paragraphs:\n",
    "        if paragraph.text.strip():\n",
    "            full_text.append(paragraph.text)\n",
    "\n",
    "    # Extract text from footers\n",
    "    for section in doc.sections:\n",
    "        footer = section.footer\n",
    "        for paragraph in footer.paragraphs:\n",
    "            if paragraph.text.strip():\n",
    "                full_text.append(paragraph.text)\n",
    "\n",
    "    # Extract text from text boxes (inline shapes)\n",
    "    for shape in doc.inline_shapes:\n",
    "        if shape.type == 3:  # 3 corresponds to a text box\n",
    "            if shape.text_frame:\n",
    "                text = shape.text_frame.text\n",
    "                if text.strip():\n",
    "                    full_text.append(text)\n",
    "\n",
    "    return '\\n'.join(full_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16771add",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_text_from_docx(docx_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495bd3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = Document(docx_path)\n",
    "\n",
    "title_size = max_size = 0\n",
    "max_size_text = title = None\n",
    "for p in doc.paragraphs:\n",
    "    style = p.style\n",
    "    if style is not None:\n",
    "        if style.name == 'Title':\n",
    "            title_size = style.font.size.pt\n",
    "            title = p.text\n",
    "            break\n",
    "        size = style.font.size\n",
    "        if size is not None:\n",
    "            if size.pt > max_size:\n",
    "                max_size = size.pt\n",
    "                max_size_text = p.text\n",
    "\n",
    "if title is not None:\n",
    "    print(f\"Title: size={title_size} text='{title}'\")\n",
    "else:\n",
    "    print(f\"max size title: size={title_size} text='{max_size_text}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745d06d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx2txt\n",
    "\n",
    "def extract_text_from_docx(docx_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts and returns all text from the specified DOCX file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract text from the DOCX file\n",
    "        text = docx2txt.process(docx_path)\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed7490e",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_text_from_docx(docx_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd073cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install docx2txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732b34f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Replace with your DOCX path\n",
    "docx_path = \"./Faseeh Curriculum Vitae.docx\"\n",
    "\n",
    "# Process the CV\n",
    "cv_data = process_cv(docx_path)\n",
    "\n",
    "# Print extracted sections\n",
    "for section, content in cv_data.items():\n",
    "    print(f\"\\n{'='*50}\\n{section.upper()}:\\n{'='*50}\")\n",
    "    print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282e79d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = create_resume_docx(cv_data)\n",
    "print(f\"Resume generated at: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b025b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "from typing import Dict\n",
    "import re\n",
    "\n",
    "def process_cv(docx_path: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Process a DOCX CV and extract structured information.\n",
    "    Args:\n",
    "        docx_path (str): Path to the DOCX file\n",
    "    Returns:\n",
    "        Dict: Dictionary containing structured CV information\n",
    "    \"\"\"\n",
    "    doc = Document(docx_path)\n",
    "    cv_data = {}\n",
    "    current_section = None\n",
    "    section_content = []\n",
    "    \n",
    "    # Define section headers and their normalized names\n",
    "    section_headers = {\n",
    "        'EDUCATION': 'education',\n",
    "        'SKILLS': 'skills',\n",
    "        'PROFESSIONAL EXPERIENCE AND INTERNSHIPS': 'experience',\n",
    "        'PROJECTS': 'projects',\n",
    "        'RESEARCH PAPERS': 'research_papers',\n",
    "        'ACHIEVEMENTS': 'achievements'\n",
    "    }\n",
    "\n",
    "    # Process contact information from first paragraph\n",
    "    first_line = doc.paragraphs[0].text.strip()\n",
    "    contact_parts = first_line.split(' | ')\n",
    "    contact_info = {\n",
    "        'email': contact_parts[0],\n",
    "        'phone': contact_parts[1],\n",
    "        'links': {\n",
    "            'linkedin': 'LinkedIn',\n",
    "            'github': 'Github',\n",
    "            'portfolio': 'Portfolio',\n",
    "            'kaggle': 'Kaggle',\n",
    "            'tableau': 'Tableau'\n",
    "        }\n",
    "    }\n",
    "    cv_data['contact_info'] = contact_info\n",
    "\n",
    "    # Initialize all sections\n",
    "    for section in section_headers.values():\n",
    "        cv_data[section] = []\n",
    "\n",
    "    # Track if we're in skills section to handle special formatting\n",
    "    in_skills_section = False\n",
    "    current_skill_category = None\n",
    "\n",
    "    for para in doc.paragraphs:\n",
    "        text = para.text.strip()\n",
    "        if not text:\n",
    "            continue\n",
    "\n",
    "        # Check if this is a section header\n",
    "        is_header = False\n",
    "        for header, section_name in section_headers.items():\n",
    "            if header in text.upper():\n",
    "                current_section = section_name\n",
    "                is_header = True\n",
    "                in_skills_section = (section_name == 'skills')\n",
    "                break\n",
    "\n",
    "        if not is_header and text:\n",
    "            if current_section == 'skills':\n",
    "                # Handle skills section specially\n",
    "                if ':' in text:\n",
    "                    # This is a skill category\n",
    "                    category, skills = text.split(':', 1)\n",
    "                    cv_data['skills'].append({\n",
    "                        'category': category.strip(),\n",
    "                        'skills': [skill.strip() for skill in skills.split(',')]\n",
    "                    })\n",
    "            else:\n",
    "                # Add content to current section\n",
    "                if current_section:\n",
    "                    cv_data[current_section].append(text)\n",
    "\n",
    "    # Clean up research papers section\n",
    "    if cv_data['research_papers']:\n",
    "        papers = []\n",
    "        current_paper = None\n",
    "        for text in cv_data['research_papers']:\n",
    "            if text.endswith(':'):\n",
    "                if current_paper:\n",
    "                    papers.append(current_paper)\n",
    "                current_paper = {'title': text[:-1], 'description': ''}\n",
    "            elif current_paper:\n",
    "                current_paper['description'] = text\n",
    "        if current_paper:\n",
    "            papers.append(current_paper)\n",
    "        cv_data['research_papers'] = papers\n",
    "\n",
    "    # Clean up achievements section\n",
    "    if cv_data['achievements']:\n",
    "        # Remove the \"ACHIEVEMENTS\" header if it's in the list\n",
    "        achievements = [ach for ach in cv_data['achievements'] if 'ACHIEVEMENTS' not in ach.upper()]\n",
    "        cv_data['achievements'] = achievements\n",
    "\n",
    "    return cv_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e8f6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    docx_path = \"./Faseeh Curriculum Vitae.docx\"\n",
    "    cv_data = process_cv(docx_path)\n",
    "    \n",
    "    # Print extracted data in a structured way\n",
    "    for section, content in cv_data.items():\n",
    "        print(f\"\\n{'='*50}\\n{section.upper()}:\\n{'='*50}\")\n",
    "        if isinstance(content, list):\n",
    "            for item in content:\n",
    "                if isinstance(item, dict):\n",
    "                    print(f\"\\n{item}\")\n",
    "                else:\n",
    "                    print(f\"\\n{item}\")\n",
    "        else:\n",
    "            print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4656ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a400de8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dc11d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
